% \begin{rem}
%   In this section we review major results in basic analysis
%   that are relevant to numerical analysis.
% \end{rem}

\section{Sequences}
\label{sec:sequences}

\begin{defn}
  A \emph{sequence} is a function on $\mathbb{N}$.
%   or $\mathbb{N}$.
   % or a countable set
   % $\{a_n\ |\ n\in \mathbb{N}^+\}$.
\end{defn}

\begin{rem}
  Whether the sequence starts from $0\in\mathbb{N}$
  or some $m\in \mathbb{N}^+$
  is a matter of convention and convenience.
\end{rem}

\begin{defn}
  The \emph{supremum and infimum of a sequence}
  $(a_n)_{n=m}^{\infty}\subset \mathbb{R}$ are respectively 
  \begin{align}
    \label{eq:supSequence}
    \sup(a_n)_{n=m}^{\infty}
    &:= \sup \{a_n: n\ge m\},
    \\
    \label{eq:infSequence}
    \inf(a_n)_{n=m}^{\infty}
    &:= \inf \{a_n: n\ge m\},
  \end{align}
  where $\sup$ and $\inf$ on the RHS
  denotes the supremum and the infimum of a set
  in Definition \ref{def:SupAndInf}. 
\end{defn}

\begin{rem}
  The unique existence of the supremum and infimum of $\mathbb{R}$
  is guaranteed by Theorem \ref{thm:completenessOfR}.
\end{rem}


\subsection{Convergence in $\mathbb{R}$}
\label{sec:convergenceOfSequenceInR}

\begin{defn}[Limit of a sequence]
  \label{def:limitOfSequence}
  A sequence $(a_n)$ in $\mathbb{R}$
  is said to \emph{converge in $\mathbb{R}$} iff
  \begin{equation}
    \label{eq:limitOfSequence}
    \exists L\in \mathbb{R} \text{ s.t. }  
    \forall \epsilon>0,\ \exists N\in \mathbb{N}, \text{ s.t. }\ 
    \forall n>N, \ |a_n-L| < \epsilon, 
  \end{equation}
  where $L$ is called the \emph{limit} of $(a_n)$, 
  written $\lim_{n\rightarrow \infty} a_n = L$
  or 
  \mbox{$a_n\rightarrow L \text{ as } n\rightarrow\infty$}, 
  and $(a_n)$ is said to \emph{converges} to $L$.
  The sequence $(a_n)$ is said to \emph{diverge}
  iff it does not converge.
\end{defn}

\begin{exm}[A story of $\pi$]
  A famous estimation of $\pi$ in ancient China
  is given by Zu, ChongZhi 1500 years ago,
  \begin{equation*}
    \pi\approx \frac{355}{113}\approx 3.14159292.
  \end{equation*}
  In modern mathematics,
   we approximate $\pi$ with a sequence
   for increasing accuracy, e.g. 
   \begin{equation}
     \label{eq:pi}
     \pi \approx 3.14159 26535 89793 \ldots
   \end{equation}
  As of March 2019, we human beings have more than 31 trillion digits
  of $\pi$.
  However, real world applications
  never use even a small fraction of the 31 trillion digits:
  \begin{itemize}\itemsep0em
  \item If you want to build a fence over your backyard swimming pool,
    several digits of $\pi$ is probably enough;
  \item in NASA, calculations involving $\pi$
    use 15 digits for Guidance Navigation and Control;
  \item if you want to compute the circumference of the entire universe
    to the accuracy of less than the diameter of a hydrogen atom,
    you need only 39 decimal places of $\pi$.
  \end{itemize}
  
 %  is good enough for estimating the length of the
 % perimeter of your backyard foundation
 % and many other common geometric calculations.

 On one hand, computational mathematics
 is judged by a metric that is different
 from that of pure mathematics;
 this may cause a huge gap
 between what needs to be done and what has been done.
%
On the other hand,
a computational mathematician cannot assume
that a fixed accuracy is good enough
for all applications.
In the approximation a number or a function, 
 she must develop theory and algorithms
 to provide the user the choice of an ever-increasing amount of accuracy, 
 so long as the user is willing
 to invest an increasing amount of computational resources.
This is one of the main motivations of infinite sequence and series.
\end{exm}
% We will derive the following example later this week.

% \begin{equation*}
%   \label{eq:pi1}
%   \pi = 4(1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+ \cdots)
%       = 4\sum_{k=0}^{\infty}(-1)^k\frac{1}{2k+1}.
% \end{equation*}

\begin{lem}
  \label{lem:seqLimitIsUnique}
  A convergent sequence has a unique limit.
\end{lem}
\begin{proof}
  Suppose a sequence $(a_{n})$ that converges to $L$
  also converges to $L_{0}\neq L.$
  For $\epsilon=\frac{|L-L_{0}|}{2}>0,$ there exists $N\in \mathbb{N}$
  such that for all $n\ge N$ we have
  $|a_{n}-L|<\epsilon$ and $|a_{n}-L_{0}|<\epsilon$. It follows that
  \begin{align*}
    |L-L_{0}|&= |L-a_{N}+a_{N}-L_{0}|\\
    &\le |a_{N}-L|+|a_{N}-L_{0}|
    < 2\epsilon = |L-L_{0}|,
  \end{align*}
  which is a contradiction.
\end{proof}

\begin{lem}
  \label{lem:subsequenceInConvergentSequenceIsConvergent}
  Any subsequence of a convergent sequence converges
  to the same limit of the sequence.
\end{lem}
\begin{proof}
  Suppose a sequence $(a_{n})$ converges to $L$.
  Then for any $\epsilon>0$, there exists $N\in \mathbb{N}$ such that
  $|a_{n}-L|<\epsilon$ for all $n\ge N$.
  Let $(a_{n_{j}})$ be a subsequence of $(a_{n})$.
  Then there exists $j_{0}\in \mathbb{N}$
  such that $n_{j_{0}}\ge N$
  and $|a_{n_{j}}-L|<\epsilon$ for all $j\ge j_{0}$,
  which implies the convergence of $(a_{n_{j}})$ to $L$.
\end{proof}

\begin{defn}
  \label{def:CauchySequence}
  A sequence $(a_n)$ is \emph{Cauchy} if
  \begin{equation}
    \label{eq:CauchySequence}
    \forall \epsilon>0, \exists N\in \mathbb{N}
    \text{ s.t. } m,n>N\ \Rightarrow\ |a_n-a_m| < \epsilon.
  \end{equation}
\end{defn}

\begin{lem}
  \label{lem:convergentSeqIsCauchy}
  Every convergent sequence in $\mathbb{R}$
  is Cauchy.
\end{lem}
\begin{proof}
  Since $(a_n)$ converges to some $L\in \mathbb{R}$,
  for any given $\epsilon>0$,
  there exists $N\in \mathbb{N}$ such that
  for all $n>N$ we have $|a_n-L|<\frac{\epsilon}{2}$.
  It follows that
  \begin{align*}
    \forall n,m>N,\  |a_n-a_m|
    &= |a_n-L+L-a_m|
    \\
    &\le |a_n-L| + |a_m-L| < \epsilon.\qedhere
  \end{align*}
%  The proof is completed by Definition \ref{def:CauchySequence}. 
\end{proof}

\begin{lem}
  \label{lem:CauchySubseqConvergence}
  If a Cauchy sequence contains a convergent subsequence,
  then the entire sequence converges to the same limit.
\end{lem}
\begin{proof}
  Suppose $(a_n)$ is a Cauchy sequence
  %in a normed space $V$
  and $(a_{n_j})$ is a subsequence converging to some $L\in \mathbb{R}$.
  It follows that
  \begin{displaymath}
    \begin{array}{l}
    \forall \epsilon>0, \exists n_0 \text{ s.t. }
    \forall m,n\ge n_0,\  |a_m-a_n|< \frac{\epsilon}{2};
    \\
    \forall \epsilon>0, \exists j_0 \text{ s.t. }
    \forall j\ge j_0,\  |a_{n_j}-L|< \frac{\epsilon}{2}.
    \end{array}
  \end{displaymath}
  Set $N=\max\{n_0, n_{j_0}\}$ and we have
  \begin{displaymath}
    \forall \epsilon>0, \forall n\ge N, 
    |a_{n}-L|\le |a_{n}-a_N|+ |a_{N}-L|
    < \epsilon,
  \end{displaymath}
  which completes the proof.
\end{proof}

\begin{lem}
  \label{lem:CauchySeqIsBounded}
  Every Cauchy sequence is bounded.
\end{lem}
\begin{proof}
  Suppose $(a_{n})$ is a Cauchy sequence. For $\epsilon=1$,
  there exists $N\in \mathbb{N}$ such that for all $m,n\ge N$ we have
  $|a_{n}-a_{m}|\le 1$. It follows that
  \begin{displaymath}
    \forall n\ge N, |a_{n}|\le |a_{N}|+|a_{n}-a_{N}|\le |a_{N}|+1.
  \end{displaymath}
  Let $M:=\max\{|a_{1}|,|a_{2}|,\cdots,|a_{N-1}|,|a_{N}|+1\}$ and
  we have $|a_{n}|\le M$ for all $n\in \mathbb{N}$.
\end{proof}

\begin{defn}
  \label{defn:monotoneSequences}
  A sequence $(a_n)_{n=m}^{+\infty} \subset \mathbb{R}$ is
  \begin{itemize}\itemsep0em
  \item \emph{monotonically increasing} iff
    $a_n\le a_{n+1}$ for each $n$; 
  \item \emph{monotonically decreasing} iff
    $a_n\ge a_{n+1}$ for each $n$. 
  \end{itemize}
  $(a_n)_{n=m}^{+\infty}$ is a \emph{monotone sequence}
  iff it is monotonically increasing or monotonically decreasing.
\end{defn}

\begin{lem}
  \label{lem:monotoneSubsequence}
  Every real sequence has a monotone subsequence.
\end{lem}
\begin{proof}
  Define $A:=\{a_{m}: 
  \forall n>m, a_{n}<a_{m}\}$. Clearly $A\subseteq (a_{n})$.

  If $\#A=+\infty$, we can choose infinitely many elements in $A$.
  Hence we take $1\le n_{1}<n_{2}<\cdots$ s.t.
  $\forall j\ge 1,\ a_{n_{j}}\in A$, 
  and then we get a subsequence $(a_{n_{j}})$,
  which is monotonically decreasing by the definition of $A$.

  If $\#A<+\infty$, then there exists some $N\in \mathbb{N}$ such that
  for any $n>N$ we have $a_{n}\notin A$. Thus
  \begin{align*}
    \forall n>N,\ \exists m>n,\text{ s.t. } a_{n}\le a_{m}.
  \end{align*}
  Hence for $n_{1}>N$, there exists $n_{2}>n_{1}$ s.t.
  $a_{n_{1}}\le a_{n_{2}}$. For $n_{2}$, there exists $n_{3}>n_{2}$ s.t.
  $a_{n_{2}}\le a_{n_{3}}$. By an easy induction, for $n_{j}$,
  there exists $n_{j+1}>n_{j}$ s.t. $a_{n_{j}}\le a_{n_{j+1}}$.
  Hence we have constructed a monotonically increasing subsequence
  $(a_{n_{j}})$.
\end{proof}

\begin{rem}
  The idea to prove Lemma \ref{lem:monotoneSubsequence} is as follows.
  Imagine that there is an infinite chain of hotels
  along the real line,
  where the $n$th hotel has height $x_n$.
  There is a sea at $+\infty$.
  A hotel is said to have the \emph{sea-view property}
  if it is higher than all hotels to the right of it.
  There are only two possibilities:
  infinitely many hotels have the sea-view property
  or finitely many hotels have the sea-view property.
  The former case corresponds to a monotonically decreasing sequence
  while
  the latter case corresponds to a monotonically increasing sequence.
\end{rem}

\begin{thm}
  \label{thm:boundedMonotoneSeqIsConvergent}
  A bounded monotone sequence is convergent.
\end{thm}
\begin{proof}
  Let $(a_{n})$ be a bounded monotone sequence.
  WLOG, we assume that $(a_{n})$ is monotonically increasing.
  Let $L:=\sup(a_{n})_{n=1}^{\infty}$, which is finite because of
  the boundedness of $(a_{n})$.
  Then for any $\epsilon>0$, there exists $N\in \mathbb{N}$ such that
  $a_{N}>L-\epsilon$. Hence for all $n\ge N,$ we have
  \begin{displaymath}
    L-\epsilon<a_{N}\le a_{n}\le L\ \Rightarrow\ |a_{n}-L|<\epsilon,
  \end{displaymath}
  which implies the convergence of $(a_{n})$ to $L$.
\end{proof}

\begin{thm}[Bolzano-Weierstrass]
  \label{thm:Bolzano-Weierstrass}
  Every bounded sequence in $\mathbb{R}$
  has a convergent subsequence.
\end{thm}
\begin{proof}
  Let $(a_{n})$ be a bounded sequence in $\mathbb{R}$.
  By Lemma \ref{lem:monotoneSubsequence},
  $(a_{n})$ has a monotone subsequence $(a_{n_{j}})$,
  which is also bounded.
  Then Theorem \ref{thm:boundedMonotoneSeqIsConvergent}
  completes the proof.
\end{proof}

\begin{rem}
  The Bolzano-Weierstrass theorem generalizes
  to finite-dimensional Euclidean spaces.
  Its generalization to infinite-dimensional metric spaces
  leads to the concept of sequential compactness
  in Section \ref{sec:sequentialCompactness}. 
\end{rem}

\begin{thm}%[Cauchy criterion]
  \label{thm:realCauchySeqConverges}
  Every Cauchy sequence in $\mathbb{R}$
  converges to a limit in $\mathbb{R}$.
\end{thm}
\begin{proof}
  By Lemma \ref{lem:CauchySeqIsBounded},
  the Cauchy sequence $(a_n)$ is bounded.
  Theorem \ref{thm:Bolzano-Weierstrass}
  implies that $(a_n)_{n\in \mathbb{N}}$ has a convergent
  subsequence $(a_{n_k})_{k\in \mathbb{N}}$.
  Then Lemma \ref{lem:CauchySubseqConvergence}
  completes the proof.
\end{proof}

\begin{thm}[Completeness of $\mathbb{R}$]
  \label{thm:completenessOfRealNumbers}
  A sequence of real numbers is Cauchy
  if and only if it is convergent.
\end{thm}
\begin{proof}
  This is a summary of
  Lemma \ref{lem:convergentSeqIsCauchy}
  and Theorem \ref{thm:realCauchySeqConverges}.
\end{proof}


\subsection{Convergence in $\overline{\mathbb{R}}$}
\label{sec:convergenceOfSequenceInRStar}

\begin{defn}
  \label{def:extendedReals}
  The \emph{extended real number system} is 
  the set $\overline{\mathbb{R}}$ that consists of the real numbers %$\mathbb{R}$
   and two symbols, % $-\infty$ and $+\infty$, 
  \begin{equation}
    \label{eq:extendedReals}
    \overline{\mathbb{R}} := \mathbb{R} \cup \{-\infty, + \infty\}, 
  \end{equation}
  such that the following rules hold:
  \begin{subequations}
    \label{eq:extendedRealsConditions}
    \begin{align}
      x\in \mathbb{R}
      &\Rightarrow \left\{
        \begin{array}{l}
          -\infty < x < +\infty; 
          \\
          x\pm\infty = \pm\infty; %, x-\infty=-\infty;
          \\
          \frac{x}{+\infty} = \frac{x}{-\infty} = 0,
        \end{array}\right.
      \\
      x>0 &\Rightarrow x\cdot(\pm\infty) = \pm \infty,
      \\
      x<0 &\Rightarrow x\cdot(\pm\infty) = \mp \infty.
    \end{align}
  \end{subequations}
\end{defn}

\begin{rem}
  $\mathbb{R}$ is a field but $\overline{\mathbb{R}}$ is not.
  Why?
\end{rem}

\begin{defn}
  An extended real number $x\in \overline{\mathbb{R}}$
  is \emph{finite} iff $x\in \mathbb{R}$
  and is \emph{infinite} otherwise.
\end{defn}

\begin{rem}
  Some of the concepts and results in Section
  \ref{sec:convergenceOfSequenceInR}
  that hold in $\mathbb{R}$
  can be generalized to hold in $\overline{\mathbb{R}}$. 
  % Theorem \ref{thm:monotoneSubsequenceInR*}
  % is an example.
  % Lemma \ref{lem:subsequenceInConvergentSequenceIsConvergent}
  % can also hold in $\overline{\mathbb{R}}$.
\end{rem}

\begin{defn}
  \label{def:divergeToInfty}
  A sequence $(a_n)\subset \mathbb{R}$
  \emph{diverges to $+\infty$ or $-\infty$},
  written $\lim_{n\rightarrow +\infty} a_n = \pm \infty$,
  iff
  \begin{equation}
    \label{eq:divergeToInfty}
    \forall M>0,\ \exists N\in \mathbb{N},\text{ s.t. }
    \forall n>N, a_{n}>M\ \text{or }a_{n}<-M.
  \end{equation}
\end{defn}

\begin{rem}
  In Definition \ref{def:divergeToInfty},
  the divergence of $(a_n)$ to $\pm \infty$
  is a special case of the more general divergence
  in Definition \ref{def:limitOfSequence}. 
  Sometimes a sequence $(a_n)$ neither diverge to $\pm \infty$
  nor converges to any number.  
\end{rem}

\begin{defn}
  \label{def:limitOfSequenceInRclosure}
  A sequence $(a_n)\subset \mathbb{R}$
  \emph{converges in $\overline{\mathbb{R}}$},
  written $\lim_{n\rightarrow \infty} a_n = L\in \overline{\mathbb{R}}$,
  iff $(a_{n})$ converges in $\mathbb{R}$ or
  diverges to $\pm \infty$. 
\end{defn}

\begin{thm}
  \label{thm:monotoneSubsequenceInR*}
  Any monotone sequence in $\overline{\mathbb{R}}$ converges in $\overline{\mathbb{R}}$.
\end{thm}
\begin{proof}
  Let $(a_{n})$ be a monotone sequence.
  WLOG, we assume that $(a_{n})$ is monotonically increasing.
  Let $A:=\sup(a_{n})_{n=1}^{\infty}\in \overline{\mathbb{R}}. $
  If $A=-\infty$, then $(a_{n})_{n=1}^{\infty}=(-\infty)_{n=1}^{\infty}$,
  which is a constant sequence and of course diverges to $-\infty$.
  If $A\in \mathbb{R}$, then $(a_{n})$ converges to $A$ by the proof of
  Theorem \ref{thm:boundedMonotoneSeqIsConvergent}.
  If $A=+\infty$, then for any $M>0$,
  there exists $N\in \mathbb{N}$ such that $a_{N}>M$.
  Hence for all $n\ge N,$ we have $a_{n}\ge a_{N}>M$.
  Definition \ref{def:limitOfSequenceInR*} implies
  that $(a_{n})$ diverges to $+\infty$.
\end{proof}

\begin{coro}
  \label{coro:theEquivalenceOfMonotoneSequenceLimit}
  Let $(a_{n})_{n=1}^{\infty}$ and $(b_{n})_{n=1}^{\infty}$
  be monotonically increasing and decreasing
  sequences in $\overline{\mathbb{R}}$, respectively. Then we have
  \begin{align}
    \lim_{n\rightarrow \infty}a_{n}&=\sup(a_{n})_{n=1}^{\infty},
    \label{eq:monotonicallyIncreasingLimit}
    \\
    \lim_{n\rightarrow \infty}b_{n}&=\inf(b_{n})_{n=1}^{\infty}.
    \label{eq:monotonicallyDecreasingLimit}
  \end{align}
\end{coro}
\begin{proof}
  This follows from the proof of
  Theorem \ref{thm:monotoneSubsequenceInR*}.
\end{proof}

\begin{thm}
  \label{thm:interchangeOfLimits}
  Consider $b_{ij}\ge 0$ in $\overline{\mathbb{R}}$ where $i,j\in\mathbb{N}$.
  If $b_{ij}$ is monotonically increasing in $i$ for each $j$
  and is monotonically increasing in $j$ for each $i$,
  then we have
  \begin{equation}
    \label{eq:interchangeOfLimits}
    \lim_{i\rightarrow\infty} \lim_{j\rightarrow\infty} b_{ij}
    = \lim_{j\rightarrow\infty} \lim_{i\rightarrow\infty} b_{ij}
  \end{equation}
  with all the indicated limits existing in $\overline{\mathbb{R}}$.
\end{thm}
\begin{proof}
  Define $a_{i}:=\lim_{j\rightarrow\infty}b_{ij}$
  and $c_{j}:=\lim_{i\rightarrow\infty}b_{ij}$. 
  Since $b_{ij}$ is monotonically increasing in $j$ for each $i$,
  Corollary \ref{coro:theEquivalenceOfMonotoneSequenceLimit}
  implies $a_{i}=\sup(b_{ij})_{j=0}^{\infty}$ for each $i\ge 0$.
  Similarly we have $c_{i}=\sup(b_{ij})_{i=0}^{\infty}$ for each $j\ge 0$.
  Since for any $i_{1}<i_{2}$, we have
  $b_{i_{1},j}\le b_{i_{2},j}$ for all $j\ge 0$, and hence
  \begin{displaymath}
    a_{i_{1}}=\sup(b_{i_{1},j})_{j=0}^{\infty}
    \le \sup(b_{i_{2},j})_{j=0}^{\infty} =a_{i_{2}},
  \end{displaymath}
  which implies that
  $(a_{i})_{i=0}^{\infty}$ is monotonically increasing. It follows that
  $\lim_{i\rightarrow\infty}a_{i}=\sup(a_{i})_{i=0}^{\infty}$
  from Corollary \ref{coro:theEquivalenceOfMonotoneSequenceLimit}.
  Similarly, we have
  $\lim_{j\rightarrow\infty}c_{j}=\sup(c_{j})_{j=0}^{\infty}.$
  Hence
  \begin{align*}
    \lim_{i\rightarrow\infty} \lim_{j\rightarrow\infty} b_{ij}
    =\lim_{i\rightarrow\infty} a_{i}
    =\sup(a_{i})_{i=0}^{\infty}
    \ge \sup(b_{ij})_{i=0}^{\infty}
    =c_{j}
  \end{align*}
  holds for all $j\ge 0$, which implies that
  \begin{align*}
    \lim_{i\rightarrow\infty} \lim_{j\rightarrow\infty} b_{ij}
    \ge \sup(c_{j})_{j=0}^{\infty}
    =\lim_{j\rightarrow\infty} c_{j}
    =\lim_{j\rightarrow\infty} \lim_{i\rightarrow\infty} b_{ij}.
  \end{align*}
  Similarly, we can also prove that
  \begin{align*}
    \lim_{i\rightarrow\infty} \lim_{j\rightarrow\infty} b_{ij}
    \le \lim_{j\rightarrow\infty} \lim_{i\rightarrow\infty} b_{ij},
  \end{align*}
  hence (\ref{eq:interchangeOfLimits}) holds.
\end{proof}

\begin{thm}
  \label{thm:limitOperation}
  Suppose $(a_{n})$ converges to $a\in \overline{\mathbb{R}}$
  and $(b_{n})\subset \mathbb{R}$ converges to $b\in \mathbb{R}$.
  Then
  \begin{enumerate}[(a)]\itemsep0em
  \item $(a_{n}\pm b_{n})$ converges to $a\pm b$.
  \item If $a\in \mathbb{R}$, then $(a_{n}b_{n})$ converges to $ab$.
  \item ($b\ne 0$ and $\forall n$, $b_{n}\ne 0$)\ $\Rightarrow$\ 
    $\lim_{n\rightarrow \infty}\frac{a_{n}}{b_{n}} = \frac{a}{b}$.
  \end{enumerate}
\end{thm}
\begin{proof}
  (a) If $a\in \mathbb{R}$, the triangle inequality
  \begin{displaymath}
    |a_{n}\pm b_{n}-(a\pm b)|\le |a_{n}-a|+|b_{n}-b|
  \end{displaymath}
  yields $\lim_{n\rightarrow \infty}(a_{n}\pm b_{n})=a\pm b$.
  If $a=\pm\infty$, (\ref{eq:extendedRealsConditions}a) yields 
  \begin{displaymath}
    \lim_{n\rightarrow\infty}(a_{n}\pm b_{n})
    =\lim_{n\rightarrow\infty}(a_{n})=a=a\pm b.
  \end{displaymath}

  (b) By Lemmas \ref{lem:CauchySeqIsBounded}
  and \ref{lem:convergentSeqIsCauchy}, 
  there exists $M\in\mathbb{R}^+$ such that $|b_{n}|\le M$ for all $n$.
  The triangle inequality yields
  \begin{align*}
    |a_{n}b_{n}-ab|
    &\le |a_{n}b_{n}-ab_{n}|+|ab_{n}-ab|\\
    &\le M|a_{n}-a|+|a||b_{n}-b|,
  \end{align*}
  which implies the convergence of $(a_{n}b_{n})$ to $ab$.

  (c) Since $b\ne 0$, there exists $N\in \mathbb{N}^{+}$ and $L>0$
  such that $|b_{n}|\ge L$ for all $n\ge N$. WLOG we assume that
  $|b_{n}|\ge L$ for all $n$. The triangle inequality yields
  \begin{align*}
    \left| \frac{a_{n}}{b_{n}}-\frac{a}{b} \right|
    &\le \frac{1}{L|b|}|a_{n}b-ab_{n}| 
    \le \frac{1}{L|b|}(|a_{n}b-ab|+|ab-ab_{n}|) \\
    &=\frac{1}{L}|a_{n}-a|+\frac{|a|}{L|b|}|b_{n}-b|,
  \end{align*}
  which implies the convergence of $(\frac{a_{n}}{b_{n}})$
  to $\frac{a}{b}$.
\end{proof}


\subsection{$\lim\sup$, $\lim\inf$, and
  the asymptotic notation}
\label{sec:limSupLimInfAsympNotatation}


\begin{defn}
  Let $\epsilon>0$ be a real number.
  Two real numbers $x,y$ are said to be \emph{$\epsilon$-close}
  iff $|x-y|\le \epsilon$.
\end{defn}

\begin{defn}
  A real number $x$ is said to be \emph{$\epsilon$-adherent}
  to a sequence $(a_n)_{n=m}^{\infty}\subset \mathbb{R}$
  iff there exists an $n\ge m$ such that
  $a_n$ is $\epsilon$-close to $x$.
  $x$ is \emph{continually $\epsilon$-adherent} to
  $(a_n)_{n=m}^{\infty}$
  iff it is $\epsilon$-adherent to $(a_n)_{n=N}^{\infty}$
  for every $N\ge m$.
\end{defn}

\begin{defn}
  \label{def:limitPointInR}
  A real number $x$ is a \emph{limit point} or \emph{adherent point}
  of a sequence $(a_n)_{n=m}^{\infty}\subset \mathbb{R}$ 
  iff it is continually $\epsilon$-adherent to
  $(a_n)_{n=m}^{\infty}$ for every $\epsilon> 0$.
\end{defn}

\begin{rem}
  Two special types of limit points are the limit superior
  and the limit inferior.
\end{rem}

\begin{defn}
  \label{def:limSupLimInf}
  The \emph{limit superior} of a sequence
  $(a_n)_{n=m}^{\infty}\subset \mathbb{R}$ is
  \begin{equation}
    \label{eq:limsup}
    \lim \sup_{n\rightarrow \infty} a_n := \inf (a_N^+)_{N=m}^{\infty}, 
  \end{equation}
  where $a_N^+=\sup(a_n)_{n=N}^{\infty}$.
  The \emph{limit inferior} of $(a_n)_{n=m}^{\infty}$ is
  \begin{equation}
    \label{eq:liminf}
    \lim \inf_{n\rightarrow \infty} a_n := \sup (a_N^-)_{N=m}^{\infty}, 
  \end{equation}
  where $a_N^-=\inf(a_n)_{n=N}^{\infty}$.
\end{defn}

\begin{rem}
  Informally, $a_N^+$ and $a_N^-$ are the supremum and infimum
  of all elements in the sequence from $a_N$ onwards.
  Each of them yields a sequence with respect to $N$.
\end{rem}

\begin{exm}
  Let $(a_n)_{n=m}^{\infty}$ be the sequence
  \begin{displaymath}
    1.1, -1.01, 1.001, -1.0001, 1.00001, \ldots
  \end{displaymath}
  Then $(a_n^+)_{n=N}^{\infty}$ is the sequence
  \begin{displaymath}
    1.1, 1.001, 1.001, 1.00001, 1.00001, \ldots
  \end{displaymath}
  and $(a_n^-)_{n=N}^{\infty}$ is the sequence
  \begin{displaymath}
    -1.01, -1.01, -1.0001, -1.0001, -1.000001, -1.000001, \ldots
  \end{displaymath}
  Hence we have
  \begin{displaymath}
    \lim \sup_{n\rightarrow \infty} a_n = 1, \qquad
    \lim \inf_{n\rightarrow \infty} a_n = -1.
  \end{displaymath}
\end{exm}

\begin{lem}[Properties of $\lim\sup$ and $\lim\inf$]
  \label{lem:qualitiesAboutLimitSuperiorAndInferior}
  The limit superior $L^+=\lim\sup_{n\rightarrow\infty} a_n$
  and the limit inferior $L^-=\lim\inf_{n\rightarrow\infty} a_n$
  of a sequence $(a_n)_{n=m}^{\infty}\subset \mathbb{R}$
  satisfy
  \begin{enumerate}[(a)]
  \item For every $x>L^+$,
    elements of the sequence are eventually less than $x$:
    \begin{displaymath}
      \forall x>L^+, \exists N\ge m \text{ s.t. }
      \forall n\ge N, a_n<x.
    \end{displaymath}
    Similarly, for every $x<L^-$,
    elements of the sequence are eventually greater than $x$:
    \begin{displaymath}
      \forall x<L^-, \exists N\ge m \text{ s.t. }
      \forall n\ge N, a_n>x; 
    \end{displaymath}
  \item For every $x<L^+$,
    there are an infinite number of elements in the sequence
    that are greater than $x$:
    \begin{displaymath}
      \forall x<L^+, \forall N\ge m,
      \exists n\ge N \text{ s.t. } a_n>x.
    \end{displaymath}
    Similarly,
    for every $x>L^-$,
    there are an infinite number of elements in the sequence
    that are less than $x$:
    \begin{displaymath}
      \forall x>L^-, \forall N\ge m,
      \exists n\ge N \text{ s.t. } a_n < x; 
    \end{displaymath}
  \item $\inf(a_n)_{n=m}^{\infty}\le L^- \le L^+ \le
    \sup(a_n)_{n=m}^{\infty}$; 
  \item Any limit point $c$ of $(a_n)_{n=m}^{\infty}$
    satisfies $L^-\le c \le L^+$; 
  \item If $L^+$ (or $L^-$) is finite,
    then it is a limit point
    of $(a_n)_{n=m}^{\infty}$; 
  \item $\lim_{n\rightarrow \infty} a_n = c$
    if and only if $L^+=L^-=c$.
    
  \end{enumerate}
\end{lem}
\begin{proof}
  (a) WLOG, we only prove the first statement.
  This statement clearly holds if $L^{+}=+\infty$.
  
  If $L^{+}=-\infty$, then for every $x>-\infty$, 
  there exists $N\ge m$ such that  $a_{N}^{+}<x$
  by Definition \ref{def:SupAndInf}
  and $L^{+}=\inf(a_{N}^{+})_{N=m}^{\infty}$.
  Then $a_{N}^{+}=\sup(a_{n})_{n=N}^{\infty}$ implies
  $a_{n}<x$ for all $n\ge N$. 
  
  If $L^{+}\in \mathbb{R}$,
  define $\epsilon:=\frac{x-L^{+}}{2}>0$ for any $x>L^{+}$. 
  Since $L^{+}=\lim \sup a_{n}=\inf(a_{N}^{+})_{N=m}^{\infty}$,
  there exists $N\ge m$ such that
  $a_{N}^{+}<L^{+}+\epsilon=\frac{x+L^{+}}{2}<x$.
  Then $a_{N}^{+}=\sup(a_{n})_{n=N}^{\infty}$ implies
  $a_{n}<x$ for all $n\ge N$.

  (b) WLOG, we only prove the first statement.
  This statement clearly holds if $L^{+}=-\infty$.
  
  If $L^{+}=+\infty$, then $a_{N}^{+}=+\infty$ for all $N\ge m$
  by the definition of $L^{+}$.
  Hence for any $x<+\infty$ and $N\ge m$,
  there exists $n\ge N$ such that
  $a_{n}>x$ by the definition of $a_{N}^{+}$.
  
  If $L^{+}\in \mathbb{R}$,
  for every $x<L^{+}$, we have $x<a_{N}^{+}$ for all $N\ge m$
  by the definition of $L^{+}$.
  Let $\epsilon=\frac{a_{N}^{+}-x}{2}>0$.
  Since $a_{N}^{+}=\sup(a_{n})_{n=N}^{\infty}$,
  there exists $n\ge N$ such that
  $a_{n}>a_{N}^{+}-\epsilon=\frac{a_{N}^{+}+x}{2}>x$.

  (c) By definitions of $L^{+}$ and $L^{-}$, it follows that
  \begin{align*}
    L^{+}&=\inf(a_{N}^{+})_{N=m}^{\infty}\le a_{m}^{+}
           =\sup(a_{n})_{n=m}^{\infty},\\
    L^{-}&=\sup(a_{N}^{-})_{N=m}^{\infty}\ge a_{m}^{-}
           =\inf(a_{n})_{n=m}^{\infty}.
  \end{align*}
  Hence it suffices to show that $L^{-}\le L^{+}$.
  For any $k\in\mathbb{N}$, 
  \begin{align*}
     a_{N}^{+}\ge a_{N+k}
    \Rightarrow
    & \inf(a_{N}^{+})_{N=m}^{\infty}
      \ge \inf(a_{N+k})_{N=m}^{\infty}
    \\ \Rightarrow
    & L^{+} \ge \inf(a_{N})_{N=m+k}^{\infty} = a_{m+k}^{-}.
  \end{align*}
  Hence $a_{N}^{-} \le L^{+}$ for all $N\ge m$ and
  \begin{displaymath}
    L^{-}=\sup(a_{N}^{-})_{N=m}^{\infty}\le L^{+}.
  \end{displaymath}

  (d) Suppose that $c>L^{+}$. By the proof of (a),
  there exists $N\ge m$ such that $a_{n}<\frac{L^{+}+c}{2}$
  for all $n\ge N$. Let $\epsilon=\frac{c-L^{+}}{2}>0$.
  Then
  \begin{displaymath}
    (\ast):\ \forall n\ge N,\ 
    c-a_{n}>c-\frac{1}{2}(L^{+}+c)=\frac{1}{2}(c-L^{+})=\epsilon.
  \end{displaymath}
  However, $c$ is a limit point of $(a_{n})_{n=m}^{\infty}$ implies
  there exists $n\ge N$ such that $|a_{n}-c|<\epsilon$,
  which contradicts ($\ast$) and hence $c\le L^{+}$.
  Similar arguments yield $c\ge L^{-}$.

  (e) If $L^{+}$ is finite, by the definition of $L^{+}$,
  for any $\epsilon>0$, there exists $n_{0}\ge m$ such that
  $L^{+} \le a_{n_{0}}^{+}<L^{+}+\epsilon$.
  For every $N\ge m$, 
  since $a_{n_{0}}^{+}\ge L^{+}>L^{+}-\epsilon$,
  there exists $n\ge \max\{N,n_{0}\}$ such that $a_{n}>L^{+}-\epsilon.$
  Hence $L^{+}-\epsilon<a_{n}\le a_{n_{0}}^{+}<L^{+}+\epsilon$, i.e.,
  $a_{n}$ is $\epsilon$-close to $L^{+}$, which implies that
  $L^{+}$ is continually $\epsilon$-adherent to
  $(a_{n})_{n=m}^{\infty}$.
  Since $\epsilon>0$ is arbitrary, we conclude that 
  $L^{+}$ is a limit point of $(a_{n})_{n=m}^{\infty}$.
  The case of $L^{-}$ can be shown similarly.

  (f) For the necessity,
  if $c=+\infty$, then for any $M>0$,
  there exists $N_{0}\ge m$ such that
  $a_{n}>M$ for all $n\ge N_{0}$. Hence
  \begin{displaymath}
    L^{-}=\sup(a_{N}^{-})_{N=m}^{\infty}
    \ge a_{N_{0}}^{-}
    =\inf(a_{n})_{n=N_{0}}^{\infty}
    \ge M.
  \end{displaymath}
  Take $M\rightarrow +\infty$ and we have $L^{-}=+\infty$,
  which implies $L^{+}=+\infty$ because of (c).
  Hence $L^{+}=L^{-}=c=+\infty.$
  The case for $c=-\infty$ can be shown similarly.

  If $c\in \mathbb{R}$, then
  $\lim_{n\rightarrow \infty}a_{n}=c$ implies that
  $c$ is a limit point of $(a_{n})_{n=m}^{\infty}$, hence
  $L^{-}\le c\le L^{+}$ from (d).
  Now for any $\epsilon>0$, there exists $N_{0}\ge m$ such that
  $|a_{n}-c|<\epsilon$ for all $n\ge N_{0}$. Hence
  \begin{displaymath}
    L^{+}=\inf(a_{N}^{+})_{N=m}^{\infty}
    \le a_{N_{0}}^{+}
    =\sup(a_{n})_{n=N_{0}}^{\infty}\le c+\epsilon.
  \end{displaymath}
  Similarly we can get $L^{-}\ge c-\epsilon$.
  Take $\epsilon\rightarrow 0^{+}$ and we have $L^{+}\le c\le L^{-}$,
  which shows that $L^{+}=L^{-}=c$.

  For the sufficiency, 
  if $c=+\infty$,
  suppose that $(a_{n})$ does not diverge to $+\infty$, i.e.,
  \begin{displaymath}
    \exists M_{0}>0, \text{ s.t. } \forall N\ge m,\
    \exists n_{0}\ge N \text{ s.t. } a_{n_{0}}\le M_{0}.
  \end{displaymath}
  Then we can take $m\le n_{1}\le n_{2}\le \cdots$, s.t. 
  $a_{n_{j}}\le M_{0}$ for all $j\ge 1$.
  Then for any $N\ge m$, there exists $j_{0}\ge 1$ such that
  $n_{j_{0}}\ge N$ and 
  \begin{displaymath}
    a_{N}^{-}=\inf(a_{n})_{n=N}^{\infty}\le a_{n_{j_{0}}}\le M_{0}.
  \end{displaymath}
  Hence $L^{-}=\sup(a_{N}^{-})_{N=m}^{\infty}\le M_{0}<+\infty$,
  which contradicts $L^{-}=c=+\infty$.
  The case of $c=-\infty$ can be shown similarly.

  If $c\in \mathbb{R}$,
  suppose that $(a_{n})$ does not converge to $c$, i.e.,
  \begin{displaymath}
    \exists \epsilon_{0}>0, \text{ s.t. } \forall N\ge m,\ 
    \exists n_{0}\ge N \text{ s.t. } |a_{n_{0}}-c|\ge \epsilon_{0}.
  \end{displaymath}
  Then we can take $m\le n_{1}\le n_{2}\le \cdots$, s.t.
  $|a_{n_{j}}-c|\ge \epsilon_{0}$ for all $j\ge 1$.
  Define the index sets
  \begin{displaymath}
    J_{1}:=\{j\ge 1:a_{n_{j}}-c\ge \epsilon_{0}\},\
    J_{2}:=\{j\ge 1:a_{n_{j}}-c\le -\epsilon_{0}\}.
  \end{displaymath}
  Then one of them must be infinite.
  WLOG we assume $J_{1}$ is infinite. Then for any $N\ge m$,
  there exists $j_{0}\in J_{1}$ such that $n_{j_{0}}\ge N$ and 
  \begin{displaymath}
    a_{N}^{+}=\sup(a_{n})_{n=N}^{\infty}
    \ge a_{n_{j_{0}}}\ge \epsilon_{0}+c.
  \end{displaymath}
  Hence $L^{+}=\inf(a_{N}^{+})_{N=m}^{\infty}\ge \epsilon_{0}+c>c$, 
  which contradicts $L^{+}=c$.
\end{proof}

\begin{lem}
  \label{lem:limSupLimInfIndependenceOnStartingIndices}
  Suppose both the limit superior and limit inferior
  exist for a sequence $(a_n)_{n=m}^{\infty}$.
  Then 
  \begin{equation}
    \label{eq:limSupLimInfIndependenceOnStartingIndices}
    \forall M>m,\
    \left\{
      \begin{array}{l}
        \lim\sup (a_n)_{n=m}^{\infty}
        = \lim\sup (a_n)_{n=M}^{\infty},\\
        \lim\inf (a_n)_{n=m}^{\infty}
        = \lim\inf (a_n)_{n=M}^{\infty}.
      \end{array}\right.
  \end{equation}
\end{lem}
\begin{proof}
  This follows from Definition \ref{def:limSupLimInf}. 
\end{proof}

\begin{rem}
  By Lemma \ref{lem:limSupLimInfIndependenceOnStartingIndices}, 
  both the limit superior and limit inferior of a sequence
  remain the same if the first $m\in\mathbb{N}^+$ items
  are removed from the sequence.
\end{rem}

\begin{lem}
  \label{lem:limitSupAndInfPreserveOrder}
  Suppose $(a_{n})_{n=m}^{\infty}\subset \mathbb{R}$
  and $(b_{n})_{n=m}^{\infty}\subset \mathbb{R}$ satisfy
  \begin{displaymath}
    \exists M\in \mathbb{N} \text{ s.t. }
    \forall n\ge M, a_{n}\le b_{n}.
  \end{displaymath}
  Then $\lim\sup a_{n}\le \lim\sup b_{n}$
  and $\lim\inf a_{n}\le \lim\inf b_{n}$.
\end{lem}
\begin{proof}
  We only prove the first inequality
  since the second can be shown similarly. 
  \begin{align*}
    &\ \forall n\ge M, a_{n}\le b_{n}
    \\
    \Rightarrow &\ 
    \sup(a_{n})_{n=M}^{\infty}\le \sup(b_{n})_{n=M}^{\infty}
    \ \Leftrightarrow\ 
    (a_{N}^{+})_{N=M}^{\infty} \le (b_{N}^{+})_{N=M}^{\infty}\\
    \Rightarrow &\  
    \inf(a_{N}^{+})_{N=M}^{\infty} \le \inf(b_{N}^{+})_{N=M}^{\infty}
    \\ \Leftrightarrow & \  
    \inf(a_{N}^{+})_{N=m}^{\infty} \le \inf(b_{N}^{+})_{N=m}^{\infty}
    \ \Leftrightarrow\ 
    \lim\sup a_{n} \le \lim\sup b_{n},
  \end{align*}
  where the second equivalence follows from
  Lemma \ref{lem:limSupLimInfIndependenceOnStartingIndices}. 
\end{proof}

\begin{thm}[Squeeze test or the sandwich theorem]
  \label{thm:squeezeTest}
  Let $(a_n)_{n=m}^{\infty}$, $(b_n)_{n=m}^{\infty}$,
  and $(c_n)_{n=m}^{\infty}$ be sequences of real numbers
  that satisfy
  \begin{displaymath}
    \exists M\in \mathbb{N} \text{ s.t. }
    \forall n\ge M, a_n\le b_n \le c_n.
  \end{displaymath}
  Suppose $(a_n)_{n=m}^{\infty}$ and $(c_n)_{n=m}^{\infty}$
  both converge to the same limit $L$.
  Then $(b_n)_{n=m}^{\infty}$ also converges to $L$.
\end{thm}
\begin{proof}
  We have
  \begin{align*}
    &\ \forall n\ge M,\ a_{n} \le b_{n}\le c_{n}
    \\ \Rightarrow
    &\ \lim\sup a_{n}\le \lim\sup b_{n}\le \lim\sup c_{n}
    \\ \Rightarrow
    &\ L\le \lim\sup b_{n}\le L
    \\ \Rightarrow
    &\ \lim\sup b_{n} =L,
  \end{align*}
  where the first step follows from
  Lemma \ref{lem:limitSupAndInfPreserveOrder} and the second step from
  Lemma \ref{lem:qualitiesAboutLimitSuperiorAndInferior}(f).
  Similarly we have $\lim\inf b_{n}=L.$ By Lemma
  \ref{lem:qualitiesAboutLimitSuperiorAndInferior}(f) again,
  we have $\lim_{n\rightarrow \infty}b_{n}=L$.
\end{proof}

\begin{rem}
  Let $(a_n)_{n=m}^{\infty}$ and $(b_n)_{n=m}^{\infty}$
  be two convergent sequences of real numbers
  that satisfy
  \begin{displaymath}
    \exists M\in \mathbb{N} \text{ s.t. }
    \forall n\ge M, a_n< b_n.
  \end{displaymath}
  Then we have
  $\lim_{n\rightarrow \infty} a_n\le \lim_{n\rightarrow  \infty} b_n$, 
  where ``$\le$'' cannot be strengthened to ``$<$''.
  In addition, replace ``$a_n< b_n$''
  with ``$a_n\le b_n$'' and we get the same conclusions.
  This is deeply connected to the difference between
  open and closed sets.
\end{rem}

\begin{ntn}[Asymptotic notation]
  \label{ntn:asymptoticNtn}
  For $g:\mathbb{R}^+\rightarrow\mathbb{R}^+$, 
  $f:\mathbb{R} \rightarrow\mathbb{R}$, 
  and $a\in [0,+\infty]$, 
  we write
  \begin{displaymath}
    f(x) = O(g(x)) \text{ as } x\to a %\in [0,+\infty]
  \end{displaymath}
  % iff there exist constants $C,C'>0$
  % and $x_0>0$ such that
  iff %if and only if
  \begin{displaymath}
    \lim\sup_{x\rightarrow a} \frac{|f(x)|}{g(x)} < \infty.
  \end{displaymath}
  In particular, we have
  \begin{displaymath}
    f(x) = o(g(x)) \text{ as } x\to a
    \ \Leftrightarrow\
    \lim_{x\rightarrow a} \frac{|f(x)|}{g(x)} = 0.
  \end{displaymath}
  We also write
  \begin{displaymath}
    f(x)=\Theta(g(x)) \text{ as } x\to a %\in [0,+\infty]
  \end{displaymath}
  iff
  \begin{displaymath}
    0< \lim\sup_{x\rightarrow a} \frac{|f(x)|}{g(x)} < \infty.
  \end{displaymath}
  When there is no danger of ambiguity,
  we often write $f(x)=O(g(x))$,
  $f(x)=o(g(x))$, and $f(x)=\Theta(g(x))$
  without the conditional $x \rightarrow a$
  in the above equations.
\end{ntn}

\begin{rem}
  Two typical cases of Notation \ref{ntn:asymptoticNtn}
  are $a=0$ and $a=+\infty$.
\end{rem}


\subsection{Series}
\label{sec:series}

\begin{defn}[Finite series]
  Let $m,n$ be integers
  and let $(a_i)_{i=m}^n$ be a finite sequence of real numbers.
  The \emph{finite series} or \emph{finite sum} associated with the sequence
  $(a_i)_{i=m}^n$ is the number $\sum_{i=m}^n a_i$
  given by the recursive formula
  \begin{equation}
    \label{eq:finiteSeries}
    \sum_{i=m}^n a_i :=
    \begin{cases}
      0 & \text{if } n< m;
      \\
      a_n + \sum_{i=m}^{n-1} a_i & \text{otherwise}.
    \end{cases}
  \end{equation}
\end{defn}

\begin{defn}%[Formal infinite series]
  \label{def:seriesFromSequence}
  A (formal) \emph{infinite series} associated with an infinite sequence $(a_n)$
   is the expression $\sum_{n=0}^{\infty} a_n$.
%   the sum of all terms of the sequence.
\end{defn}

\begin{defn}
  \label{def:sequenceOfPartialSums}
  The $n$th term of the \emph{sequence of partial sums}
  $(S_n)_{n=0}^{\infty}$ associated with a formal infinite series
  $\sum_{i=0}^{\infty} a_i$
  is the sum of the sequence $(a_i)$ from $a_0$ to $a_n$
  \begin{equation}
    \label{eq:sequenceOfPartialSums}
    S_n = \sum_{i=0}^{n} a_i.
  \end{equation}
\end{defn}

\begin{defn}
  A formal infinite series is said to be \emph{convergent}
  and \emph{converge} to $L$
  if its sequence of partial sums converges to some limit $L$.
  In this case we write $L=\sum_{n=0}^{\infty} a_n$
  and call $L$ the \emph{sum of the infinite series}.
\end{defn}

\begin{lem}
  \label{lem:seriesConvergeNecessity}
  A convergent infinite series $\sum_{n=0}^{\infty}a_{n}$ of real numbers
  must have $\lim_{n\rightarrow \infty}a_{n}=0$.
\end{lem}
\begin{proof}
  Assume that the series converges to $L\in \mathbb{R}$.
  For $S_{n}$ in \eqref{eq:sequenceOfPartialSums}, 
  we have $a_{n}=S_{n}-S_{n-1}$ for all $n$. 
  Then Theorem \ref{thm:limitOperation} yields
  $\lim_{n\rightarrow \infty}a_{n}=L-L=0$.
\end{proof}

\begin{defn}
  A formal infinite series is said to \emph{diverge}
  or be \emph{divergent}
  if its sequence of partial sums does not converge.
\end{defn}

\begin{rem}
  We do not assign any value to a divergent infinite series.
\end{rem}

\begin{lem}[Cauchy criterion]
  \label{lem:seriesConvergeCauchy}
  An infinite series $\sum_{n=0}^{\infty} a_n$ of real numbers
  is convergent if and only if
  \begin{equation}
    \label{eq:seriesConvergeCauchy}
    \forall \epsilon>0,\ \exists N\in \mathbb{N} \text{ s.t. }
    \forall p,q\ge N, \ \left|\sum_{n=p}^q a_n\right| \le \epsilon.
  \end{equation}
\end{lem}
\begin{proof}
  $\sum_{n=0}^{\infty}a_{n}$ is convergent if and only if
  its sequence $(S_{n})$ of partial sums
  in Definition \ref{def:sequenceOfPartialSums}
  is convergent, if and only if $(S_{n})$ is Cauchy
  by Theorem \ref{thm:completenessOfRealNumbers}, if and only if
  \begin{displaymath}
    \forall \epsilon>0,\ \exists N\in \mathbb{N} \text{ s.t. }
    \forall p,q\ge N, \ |S_{q}-S_{p-1}|
    =\left|\sum_{n=p}^q a_n\right| \le \epsilon. \qedhere
  \end{displaymath}
\end{proof}

\begin{thm}[Comparison test]
  \label{thm:comparisonTest}
  Let $\sum a_n$ be a series where $a_n\ge 0$ for all $n$.
  \begin{enumerate}[(i)]\itemsep0em
  \item If $\sum a_n$ converges and $|b_n|\le a_n$ for all $n$,
    then $\sum b_n$ converges.
  \item If $\sum a_n=+\infty$ and $b_n \ge a_n$ for all $n$,
    then \mbox{$\sum b_n=+\infty$}.
  \end{enumerate}
\end{thm}
\begin{proof}
  For any $p,q\in\mathbb{N}$, we have
  \begin{displaymath}
    \begin{array}{l}
    \left| \sum_{n=p}^{q}b_{n} \right|
    \le  \sum_{n=p}^{q}|b_{n}| \le \sum_{n=p}^{q}a_{n}
    =\left| \sum_{n=p}^{q}a_{n} \right|.
    \end{array}
  \end{displaymath}
  Then (i) follows from Lemma \ref{lem:seriesConvergeCauchy}. 

  By Definition \ref{def:divergeToInfty}
  and the condition
  $\sum_{j=1}^{n}b_{j}\ge \sum_{j=1}^{n}a_{j}$ for all $n\in \mathbb{N}^+$,
  the divergence of $(\sum_{j=1}^{n}a_{j})_{n=1}^{\infty}$ to $+\infty$
  implies the divergence of
  $(\sum_{j=1}^{n}b_{j})_{n=1}^{\infty}$ to $+\infty$.
\end{proof}

\begin{thm}
  \label{thm:geometricSeries}
  The \emph{geometric series} $\sum_{n=0}^{\infty} x^n$
  diverges to $+\infty$ if $x\ge 1$
  and converges to $\frac{1}{1-x}$ if $x\in [0,1)$.
\end{thm}
\begin{proof}
  For $x=1$, the geometric series
  $\sum_0^{\infty} x^n = 1 + 1+\cdots = \infty$.
  The case of $x>1$ follows from the comparison test.
  For $x\in[0,1)$,
  $\sum_0^{\infty} x^n = \frac{1-x^{n+1}}{1-x}$
  and the conclusion follows.
\end{proof}

\begin{defn}
  \label{def:absoluteConvergence}
  An infinite series $\sum_{n=0}^{\infty} a_n$
  is \emph{absolutely convergent}
  iff the series $\sum_{n=0}^{\infty} |a_n|$ is convergent.
\end{defn}

\begin{lem}
  \label{lem:absolutelyConvergenceSeriesConverge}
  An absolutely convergent infinite series is convergent.
\end{lem}
\begin{proof}
  Since $|a_{n}|\ge 0$,  $\sum |a_{n}|$ converges and
  $|a_{n}|\le |a_{n}|$ for all $n$, the proof is completed by
  (i) in Theorem \ref{thm:comparisonTest}.
\end{proof}

\begin{thm}[Ratio test]
  \label{thm:ratioTest}
  A series $\sum a_n$ of nonzero real numbers 
  \begin{enumerate}[(i)]\itemsep0em
  \item converges absolutely if
    ${\lim\sup}_{n\rightarrow\infty}|\frac{a_{n+1}}{a_n}|<1$;
  \item diverges if ${\lim\inf}_{n\rightarrow\infty}|\frac{a_{n+1}}{a_n}|>1$. 
  \end{enumerate}
  Otherwise, this test gives no information
  about the convergence of $\sum a_n$.
\end{thm}
\begin{proof}
  Write $\alpha:={\lim\sup}_{n\rightarrow\infty}|\frac{a_{n+1}}{a_n}|$.
  Choose $\epsilon>0$ such that $\alpha+\epsilon\in(0,1)$.
  Then Lemma \ref{lem:qualitiesAboutLimitSuperiorAndInferior}(a) implies
  \begin{displaymath}
    \begin{array}{l}
    \exists N\in \mathbb{N}^{+} \text{ s.t. }
    \forall n\ge N,
    \sup_{n\ge N}\{\left|\frac{a_{n+1}}{a_{n}}\right|\}<\alpha+\epsilon.
    \end{array}
  \end{displaymath}
  Hence $|\frac{a_{n+1}}{a_{n}}|<\alpha+\epsilon$ for all $n\ge N$,
  which implies that
  \begin{displaymath}
    \forall n\ge N,\ 
    |a_{n}|=\left|\frac{a_{n}}{a_{n-1}}\cdot \frac{a_{n-1}}{a_{n-2}}
     \cdots \frac{a_{N+1}}{a_{N}}\cdot a_{N}  \right|
    <|a_{N}|(\alpha+\epsilon)^{n-N}.
  \end{displaymath}
  Then (i) follows from Theorem \ref{thm:comparisonTest}
  by comparing $(|a_n|)$ with the geometric series.
  (ii) can be shown similarly.
\end{proof}

\begin{thm}[Root test]
  \label{thm:rootTest}
  A series $\sum a_n$ of real numbers
  \begin{enumerate}[(i)]\itemsep0em
  \item converges absolutely if $\alpha<1$;
  \item diverges if $\alpha>1$,
  \end{enumerate}
  where $\alpha={\lim\sup}_{n\rightarrow\infty}|a_n|^{\frac{1}{n}}$.
  Otherwise, this test gives no information
  about the convergence of $\sum a_n$.
\end{thm}
\begin{proof}
  Since $\alpha<1$, we can choose $\epsilon>0$ so that $\alpha+\epsilon\in (0,1)$.
  Lemma \ref{lem:qualitiesAboutLimitSuperiorAndInferior}(a) gives
  \begin{displaymath}
    \begin{array}{l}
    \exists N\in \mathbb{N}^+ \text{ s.t. }
    \forall n\ge N,
    \ \sup_{n\ge N}\{ |a_n|^{\frac{1}{n}}\} < \alpha+\epsilon.
    \end{array}
  \end{displaymath}
  Hence $|a_n|<(\alpha+\epsilon)^n$ for all $n\ge N$.
  The comparison test with the geometric series yields (i).

  We prove (ii) by contradiction.
  Suppose $\sum a_{n}$ converges.
  Then Lemma \ref{lem:seriesConvergeNecessity}
  yields $\lim a_{n}=0$.
  By Lemma \ref{lem:qualitiesAboutLimitSuperiorAndInferior}(b),
  there exists a subsequence $a_{n_{j}}$ such that
  $|a_{n_{j}}|^{\frac{1}{n_{j}}}>1$, i.e., $|a_{n_{j}}|>1$ for all $j$.
  But Lemma \ref{lem:subsequenceInConvergentSequenceIsConvergent} implies
  $\lim_{j\rightarrow\infty} a_{n_{j}}=\lim_{n\rightarrow\infty}a_{n}=0$,
  which contradicts $|a_{n_{j}}|>1$ for all $j$.
\end{proof}

\begin{rem}
  The root test applies whenever the ratio test applies.
  However, the converse is not true.
\end{rem}

\begin{thm}[Integral test]
  \label{thm:integralTest}
  Let $f:[0, \infty)\rightarrow \mathbb{R}^+\cup\{0\}$
   be a monotonically decreasing function,
   i.e., $x\ge y$ implies $f(x)\le f(y)$.
  Then the series $\sum_{n=0}^{\infty}f(n)$ is convergent if and only if
   $\sup_{N\in\mathbb{N}^+}\int_0^N f$ is finite.
\end{thm}
\begin{proof}
  Since $f$ is monotonically decreasing and non-negative,
  the comparison test yields
  \begin{displaymath}
    (*):\quad
    \forall N\in \mathbb{N}^{+},\ 
    \sum_{n=1}^{N}f(n)\le \int_{0}^{N}f \le \sum_{n=0}^{N-1}f(n).
  \end{displaymath}
  
  For necessity, suppose $\sum_{n=0}^{\infty}f(n)=L<+\infty$.
  Then 
  \begin{displaymath}
    \forall N\in \mathbb{N}^+,\ 
    \int_{0}^{N}f\le\sum_{n=0}^{N-1}f(n)\le\sum_{n=0}^{\infty}f(n)=L,
  \end{displaymath}
  where the first inequality follows from $(*)$
  and the second from $f\ge 0$. 
  Hence for any $T\in\mathbb{N}^+$, there exists $N_{0}\in \mathbb{N}$ such that
  $T\le N_{0}$ and thus $\int_{0}^{T}f\le \int_{0}^{N_{0}}f\le L$.

  For sufficiency, there exists $M>0$ such that
  $\int_{0}^{N}f\le M$ for all $N\in \mathbb{N}$.
  Then we have
  \begin{displaymath}
    \forall N\in \mathbb{N},\ 
    \sum_{n=0}^{N}f(n)=\sum_{n=1}^{N}f(n)+f(0)
    \le \int_{0}^{N}f+f(0)
    \le M+f(0),
  \end{displaymath}
  where the first inequality follows from $(*)$.
  By Definition \ref{defn:monotoneSequences}
  and $f\ge 0$, 
  the sequence $(\sum_{n=0}^{N}f(n))_{N\in \mathbb{N}}$
  is monotonically decreasing.
  The rest follows from
  Theorem \ref{thm:boundedMonotoneSeqIsConvergent}.
\end{proof}

\section{Scalar functions
  $\mathbb{R}\rightarrow\mathbb{R}$}
\label{sec:cont-funct-on-R}

\begin{defn}
  A \emph{scalar function} is a function
   whose range is a subset of $\mathbb{R}$.
\end{defn}

\begin{defn}[Limit of a scalar function]
  \label{def:limitOfAScalarFunc}
  Consider a function $f: I\rightarrow \mathbb{R}$
   with $I(c,r)=(c-r,c)\cup (c,c+r)$.
  The \emph{limit} of $f(x)$  exists
   as $x$ approaches $c$, written
     $\lim_{x\rightarrow c} f(x) = L$,
  iff 
  \begin{equation}
 \forall \epsilon>0, \exists \delta>0, \text{ s.t. }
    \forall x\in I(c,\delta),\ |f(x)-L|<\epsilon.
  \end{equation}
\end{defn}

\begin{rem}
  The notation in Definition \ref{def:limitOfAScalarFunc}
  reads ``as $x$ gets closer to $c$,
 $f(x)$ gets closer to $L$.''
How close is close?
As close as you wish.
This idea is packaged in the $\epsilon-\delta$ technique.
\end{rem}

\begin{exm}
  \label{exm:limitForShowingEpsDelta}
   We show $\lim_{x\rightarrow 2}\frac{1}{x}= \frac{1}{2}$ as follows.
   If $\epsilon\ge \frac{1}{2}$,
   choose $\delta=1$.
   Then $x\in(1,3)$ implies
   $\left|\frac{1}{x}-\frac{1}{2}\right|<\frac{1}{2}$
   since $\frac{1}{x}-\frac{1}{2}$
   is a monotonically decreasing function
   with its supremum at $x=1$.

  If $\epsilon\in (0,\frac{1}{2})$,
   choose $\delta=\epsilon$.
  Then $x\in (2-\epsilon,2+\epsilon)\subset
  (\frac{3}{2},\frac{5}{2})$.
  Hence
  $\left|\frac{1}{x}-\frac{1}{2}\right|=\frac{|2-x|}{|2x|}<|2-x|<\epsilon$.
  The proof is completed by Definition \ref{def:limitOfAScalarFunc}.
\end{exm}

\begin{rem}
  The philosophy in Example \ref{exm:limitForShowingEpsDelta}
  is that two functions have the same limit
  if their difference can be shown to be as small as you wish.
\end{rem}

% \begin{defn}
%   The \emph{open ball} centered at $P_0\in \mathbb{R}^n$
%    with radius $r>0$
%    is the point set
%    \begin{equation}
%      \label{eq:unitBall}
%      {\cal B}(P_0, r) = 
%      \left\{P \ \bigl|\ |P-P_0| < r\bigr.\right\}.
%    \end{equation}
%   It is an \emph{open interval} in 1D
%    and an \emph{open disk} in 2D.

%   The open ball without the center is denoted by
%    \begin{equation}
%      {\cal B}_0(P_0, r) = {\cal B}(P_0, r) \setminus
%      \{P_0\}.
%    \end{equation}
% \end{defn}

% \begin{defn}
%   \label{def:extremum}
%   $f: \mathbb{R}^n\rightarrow\mathbb{R}$ has a
%   \emph{local maximum} at $P_0\in \mathbb{R}^n$
%   iff
%   \begin{equation}
%     \label{eq:localMax}
%     \exists r>0, \text{ s.t. } \forall P \in{\cal B}(P_0, r),
%     \qquad f(P)\le f(P_0).
%   \end{equation}
%   Changing $\le$ to $\ge$ in (\ref{eq:localMax})
%    yields a \emph{local minimum}.\\
%   An \emph{extremum} is either a maximum or minimum.
% \end{defn}

% \begin{defn}
%   $P_0$ is
%   a \emph{boundary point} of a point set ${\cal U}$
%   iff $\forall r>0$, $\exists P\in {\cal B}(P_0,r)$
%   s.t. $P\not\in {\cal U}$.
%  \end{defn}

%  \begin{defn}
%   A set is an \emph{open set} if it contains
%    none of its boundary points.
%  \end{defn}

%  \begin{defn}
%    A set is a \emph{closed set} if it contains
%     all of its boundary points.
%  \end{defn}

%  \begin{defn}
%    A point set ${\cal U}\subseteq \mathbb{R}^n$
%     is \emph{bounded}
%     iff ${\cal U}\subseteq {\cal B}(P_0, r)$
%     for some $P_0\in \mathbb{R}^n$ and $r>0$.
%  \end{defn}

% \begin{defn}[Limit of a scalar function with multiple variables]
% \label{def:limit}
%   The \emph{limit} of a function
%   $f: {\cal B}_0(P_0,r)\rightarrow \mathbb{R}$
%   exists
%   as $P$ approaches $P_0$, written
% $     \lim_{P\rightarrow P_0} f(P) = L$,
%   iff
%    \begin{equation}
%      \begin{split}
%    \forall \epsilon>0,\ 
%    \forall \text{ paths } P\rightarrow P_0,\ 
%    \exists \delta>0, \text{ s.t. }\\
%    \forall P\in {\cal B}(P_0,\delta),\qquad
%    |f(P)-L|<\epsilon.
%      \end{split}
%    \end{equation}
% \end{defn}

% \begin{frm}
%   The limit of $f: {\cal B}_0(P_0,r)\rightarrow \mathbb{R}$
%    does not exist at $P_0$
%    if there exists two different paths $P\rightarrow P_0$
%    and $P \leadsto P_0$ s.t.
%    \begin{equation}
%      \bigl(\lim_{P\rightarrow P_0} f(P) = L_1\bigr)
%      \ne \bigl(L_2 = \lim_{P\leadsto P_0} f(P)\bigr).
%    \end{equation}
% \end{frm}

% \begin{thm}[The squeeze theorem]
%   If $\exists r>0$ s.t.\\
%   $\forall P\in{\cal B}_0(P_0,r)$,
%    $f(P)\le g(P)\le h(P)$,
%    then
%    \begin{equation}
%      \left(\lim_{P\rightarrow P_0}f(P)=\lim_{P\rightarrow P_0}h(P)=L\right)
%      \ \Rightarrow \ 
%      \lim_{P\rightarrow P_0} g(P) = L.
%    \end{equation}
% \end{thm}

% \begin{defn}
%   \emph{Big O notation} describes the limiting behavior
%   of a function
%   in terms of another function.
%   Given  $f,g: \mathbb{R}\rightarrow \mathbb{R}$,
%   \begin{subequations}
%     \begin{align}
%       f(h) = O(g(h))\ &\Leftrightarrow\ 
%       \lim_{h\rightarrow 0}\frac{f(h)}{g(h)} = L \ne 0
%       \\
%       f(h) = o(g(h))\ &\Leftrightarrow\ 
%       \lim_{h\rightarrow 0}\frac{f(h)}{g(h)} = 0
%     \end{align}
%   \end{subequations}
% \end{defn}

% \begin{exm}
%   We will be dealing with polynomial of $h$
%   and determining convergence is very easy in this case.
% \end{exm}

\subsection{Continuous scalar functions}
\label{sec:cont-scal-funct}

\begin{defn}
  \label{def:continuousScalarFunc}
  $f: \mathbb{R}\rightarrow \mathbb{R}$
  is \emph{continuous} at $c$
  iff
   \begin{equation}
     \label{eq:continuous}
     \lim_{x\rightarrow c} f(x) = f(c).
   \end{equation}
\end{defn}

\begin{defn}
  \label{def:continuousFuncOnR}
  A scalar function $f$ is \emph{continuous on} $(a,b)$,
     written \mbox{$f\in{\cal C}(a,b)$}, 
     if (\ref{eq:continuous}) holds  $\forall x\in (a,b)$.
\end{defn}

% \begin{defn}
%   $f: \mathbb{R}^n\rightarrow \mathbb{R}$
%   is \emph{continuous} at $Q$
%   iff
%    \begin{equation}
%      \label{eq:continuous}
%      \lim_{P\rightarrow Q} f(P) = f(Q).
%    \end{equation}
%    $f$ is \emph{continuous on} a point set ${\cal U}$
%     if (\ref{eq:continuous}) holds  $\forall Q\in {\cal U}$.
% \end{defn}

\begin{thm}[Extreme values]
  \label{thm:extremeValues}
  A continuous function $f:[a,b]\rightarrow\mathbb{R}$
  attains its maximum at some point \mbox{$x_{\max}\in [a,b]$}
  and its minimum at some point $x_{\min}\in [a,b]$.
\end{thm}

\begin{thm}[Intermediate value]
  \label{thm:intermediateValue}
  A scalar function $f\in {\cal C}[a,b]$ satisfies
  \begin{equation}
    \label{eq:intermediateValue}
    \forall y\in \left[m, M \right],\ 
    \exists \xi\in[a,b], \text{ s.t. }
    y=f(\xi)
  \end{equation}
  where  $m=\inf_{x\in[a,b]} f(x)$ and
   $M=\sup_{x\in[a,b]} f(x)$.
\end{thm}

\begin{rem}
Theorem \ref{thm:intermediateValue}
 states that a continuous function assumes
 all values between $f(a)$ and $f(b)$
 on a closed interval $[a,b]$.
\end{rem}

\begin{defn}
  \label{def:uniformlyContinuousScalar}
  Let $I=(a,b)$. A function $f: I\rightarrow \mathbb{R}$
  is \emph{uniformly continuous} on $I$
  iff
   \begin{equation}
     \label{eq:uniformlyContinuous}
     \begin{array}{l}
     \forall \epsilon>0, \exists \delta>0,\text{ s.t. }
     \\
     \forall x,y\in I,\ 
     |x-y|<\delta \Rightarrow |f(x)-f(y)|<\epsilon.
     \end{array}
   \end{equation}
\end{defn}

\begin{exc}
  Show that, on $(a,\infty)$,
  $f(x)=\frac{1}{x}$ is uniformly continuous
   if $a>0$ 
   and is not so if $a=0$.
\end{exc}
\begin{proof}
  If $a>0$, then
  $|f(x)-f(y)|=\frac{|x-y|}{xy} %< \frac{\delta}{xy}\
  <\frac{|x-y|}{a^2}$.\\
  Hence 
  $\forall \epsilon>0, \exists \delta=a^2 \epsilon$,
  s.t. \\$|x-y|<\delta \Rightarrow
  |f(x)-f(y)|<\frac{|x-y|}{a^2}<\frac{a^2 \epsilon}{a^2}=\epsilon$.

  If $a=0$, negating the condition
   of uniform continuity,
   i.e. eq. (\ref{eq:uniformlyContinuous}), yields
  $\exists \epsilon>0$ s.t. $\forall\delta>0$
   $\exists x,y>0$ s.t.
   $(|x-y|<\delta) \wedge (|\frac{1}{x}-\frac{1}{y}|\ge \epsilon)$.

   We prove a stronger version:
  $\forall \epsilon>0$, $\forall\delta>0$
   $\exists x,y>0$ s.t.
   $(|x-y|<\delta) \wedge (|f(x)-f(y)|\ge \epsilon)$.

%  Set $\epsilon=\frac{1}{3}$.
  If $\delta\ge \frac{1}{2\epsilon}$,
   choose $x=\frac{1}{2\epsilon}$, $y=\frac{1}{4\epsilon}$.
  This choice satisfies $|x-y|<\delta$
   since $x-y=\frac{1}{4\epsilon}<\frac{1}{2\epsilon}\le \delta$.
  However, $|f(x)-f(y)|=\frac{|x-y|}{xy}=2\epsilon>\epsilon$.

  If $\delta< \frac{1}{2\epsilon}$,
   then $2\epsilon\delta<1$.
  Choose $x\in (0, \epsilon\delta^2)$
   and $y\in (2\epsilon\delta^2, \delta)$.
  This choice satisfies $|x-y|<\delta$
   and $|x-y|>\epsilon\delta^2$.
  However, 
   $|f(x)-f(y)|=\frac{|x-y|}{xy}>\frac{\epsilon\delta^2}{xy}
   > \frac{1}{y} > \frac{1}{\delta} > 2\epsilon> \epsilon$.
\end{proof}

\begin{exc}
  On $(a,\infty)$,
  $f(x)=\frac{1}{x^2}$ is uniformly continuous
   if $a>0$ 
   and is not so if $a=0$.
\end{exc}
\begin{solution}
  Case 1 ($a>0$):
  The function $f(x)=\frac{1}{x^2}$ is uniformly continuous
  on $(a,\infty)$ iff
  \begin{align*}
    \forall\epsilon>0,&\exists\delta>0\text{ s.t. }
                        \forall x,y \in(a,\infty),\\
    &|x-y|<\delta\Rightarrow|f(x)-f(y)|<\epsilon.
  \end{align*}

  Since $a>0$ and $x,y \in (a,\infty)$,
  we have $|x|=x>0$ and $|y|=y>0$.  Thus,
\[
|f(x)-f(y)|
=|x-y|\left(\frac{1}{x^2y}+\frac{1}{xy^2}\right).
\]

For any given $\epsilon>0$,
choose $\delta=\frac{a^3\epsilon}{2}$.
Then we have
\begin{displaymath}
  x>a\Rightarrow\frac{1}{x}<\frac{1}{a};\ \ 
  y>a\Rightarrow\frac{1}{y}<\frac{1}{a}.
\end{displaymath}

It follows that
\[
 |x-y|<\delta \ \Rightarrow\ 
|f(x)-f(y)|
<\delta\left(\frac{1}{a^3}+\frac{1}{a^3}\right)
=\frac{2\delta}{a^3}=\epsilon.
\]
Hence $f(x)=\frac{1}{x^2}$ is uniformly continuous on $(a,\infty)$ if
$a>0$.

Case 2 ($a=0$):
The function $f(x)=\frac{1}{x^2}$ is not uniformly continuous
on $(0,\infty)$ iff
\begin{align*}
  \exists\epsilon>0\text{ s.t. } & \forall\delta>0,
  \exists x,y \in (0,\infty)\text{ s.t. }\\
  &|x-y|<\delta\Rightarrow |f(x)-f(y)|\geq\epsilon.
\end{align*}

We prove a stronger statement:
\begin{align*}
  \forall\epsilon>0 & \forall\delta>0,\exists x,y \in (0,\infty)
  \text{ s.t. } \\
  &|x-y|<\delta\Rightarrow |f(x)-f(y)|\geq\epsilon.
\end{align*}

If $\delta\geq\frac{1}{2\sqrt{\epsilon}}$,
 choose
 $x=\frac{1}{\sqrt{4\epsilon}}=\frac{1}{2\sqrt{\epsilon}}$ and $y=\frac{1}{\sqrt{9\epsilon}}=\frac{1}{3\sqrt{\epsilon}}$.
This choice satisfies $|x-y|< \delta$.
However,
\begin{align*}
  |f(x)-f(y)| &=
                \frac{|x-y|}{xy}\left(\frac{1}{x}+\frac{1}{y}\right)\\
 &= \frac{\frac{1}{6\sqrt{\epsilon}}}{\left(\frac{1}{2\sqrt{\epsilon}}\right)\left(\frac{1}{3\sqrt{\epsilon}}\right)}(2\sqrt{\epsilon}+3\sqrt{\epsilon})
  \\
&= \frac{1}{\sqrt{\epsilon}}\,\epsilon(5\sqrt{\epsilon})
= 5\epsilon >\epsilon
\end{align*}

If $\delta<\frac{1}{2\sqrt{\epsilon}}$,
 choose $x$ and $y$ so that $0<x<\sqrt{\epsilon}\delta^2$
 and $2\sqrt{\epsilon}\delta^2<y<\delta$.
Then $0<x<y<\delta$ implies $\frac{1}{x^2y^2}>\frac{1}{\delta^4}$
 and $x+y> 2\sqrt{\epsilon}\delta^2$.
Also, $2\sqrt{\epsilon}\delta<1$ and
 $2\sqrt{\epsilon}\delta^2-\sqrt{\epsilon}\delta^2<|x-y|<\delta \Rightarrow \sqrt{\epsilon}\delta^2<|x-y|<\delta$.
Altogether, these imply
\begin{align*}
|f(x)-f(y)| = \frac{|x-y|(x+y)}{x^2y^2}
> \frac{\sqrt{\epsilon}\delta^2 2\sqrt{\epsilon}\delta^2}{\delta^4}
=2\epsilon>\epsilon.
\end{align*}
Hence $f(x)=\frac{1}{x^2}$ is not uniformly continuous on $(a,\infty)$ if
$a=0$.
\end{solution}

\begin{thm}
  Uniform continuity implies continuity
   but the converse is not true.
\end{thm}
\begin{proof}
  exercise.
\end{proof}

\begin{lem}
  \label{lem:uniformContExtension}
  $f: \mathbb{R}\rightarrow \mathbb{R}$
  is uniformly continuous on $(a,b)$
  iff it can be extended
  to a continuous function $\tilde{f}$ on $[a,b]$.
\end{lem}

\begin{thm}[Uniform continuity]
  \label{thm:uniformContinuityScalarFunc}
  A continuous function $f: [a,b]\rightarrow \mathbb{R}$
  is uniformly continuous.
\end{thm}


\subsection{Differentiation of scalar functions}
\label{sec:diff-funct}

\begin{defn}
  \label{def:derivative}
  The \emph{derivative}
   of a function $f: \mathbb{R}\rightarrow \mathbb{R}$
   at $a$ is the limit
   \begin{equation}
     \label{eq:derivative}
     f'(a)=\lim_{h\rightarrow 0} \frac{f(a+h)-f(a)}{h}.
   \end{equation}
  If the limit exists, $f$ is \emph{differentiable} at $a$.
\end{defn}

\begin{exm}
  For the power function \mbox{$f(x)=x^{\alpha}$}, 
   we have $f'=\alpha x^{\alpha-1}$
   due to
   Newton's generalized binomial theorem, 
  \begin{equation*}
    (a+h)^{\alpha}=\sum_{n=0}^{\infty}{\alpha \choose n}a^{\alpha-n}h^n.
  \end{equation*}
\end{exm}

\begin{rem}
  Think about the three cases $\alpha=\frac{1}{2}$,
  $\alpha=1$, $\alpha=2$.
  If $x$ is time and $f(x)$ measures how knowledgeable you are.
  You probably want to have $f(x)=x^2$ rather than
  $f(x)=x^{\frac{1}{2}}$.
  The reason $f(x)=x^2$ is much better is that
  its rate of increase also increases.
\end{rem}

\begin{defn}
  A function $f(x)$ is
   $k$ times \emph{continuously differentiable}
   on $(a,b)$
   iff $f^{(k)}(x)$ exists on $(a,b)$ and is itself continuous.
  The set or space of all such functions on $(a,b)$
   is denoted by ${\cal C}^k(a,b)$.
  In comparison,
   ${\cal C}^k[a,b]$ is the space of functions $f$
   for which $f^{(k)}(x)$ is bounded and uniformly continuous on $(a,b)$.
\end{defn}

\begin{thm}
  \label{thm:continuousImpliesBoundedness}
  A scalar function $f$ is bounded on $[a,b]$
   if $f\in {\cal C}[a,b]$.
\end{thm}

\begin{thm}
  \label{thm:Fermat}
  If $f:(a,b)\rightarrow\mathbb{R}$
   assumes its maximum or minimum at $x_0\in(a,b)$
   and $f$ is differentiable at $x_0$,
   then $f'(x_0)=0$.
\end{thm}
\begin{proof}
  Suppose $f'(x_0)>0$. Then we have
  \begin{displaymath}
    f'(x_0)=\lim_{x\rightarrow x_0} \frac{f(x)-f(x_0)}{x-x_0}>0.
  \end{displaymath}
  The definition of a limit implies
  \begin{displaymath}
   \exists \delta>0 \text{ s.t. } 
   a<x_0-\delta<x_0+\delta<b,
  \end{displaymath}
   which, together with $|x-x_0|<\delta$, 
   implies
   $\frac{f(x)-f(x_0)}{x-x_0}>0$.
  This is a contradiction to $f(x_0)$ being a maximum
  when we choose $x\in(x_0,x_0+\delta)$.
\end{proof}

\begin{thm}[Rolle's]
  \label{thm:Rolles}
  If a function $f:\mathbb{R}\rightarrow\mathbb{R}$
   satisfies
   \begin{enumerate}[(i)]\itemsep0em
   \item $f\in {\cal C}[a,b]$ and $f'$ exists on $(a,b)$,
   \item $f(a)=f(b)$,
   \end{enumerate}
  then $\exists x\in (a,b)$ s.t. $f'(x)=0$.
\end{thm}
\begin{proof}
  % Theorem 18.1 in Ross states that
  %  if $f$ is continuous on a closed interval,
  %  then $f$ is bounded
  %  and assumes its maximum and minimum values
  %  on $[a,b]$.
  % By Theorem \ref{thm:continuousImpliesBoundedness},
  %  $f$ is bounded.
  By Theorem \ref{thm:intermediateValue},
   all values between $\sup f$ and $\inf f$
   will be assumed.
  If $f(a)=f(b)=\sup f=\inf f$,
   then $f$ is a constant on $[a,b]$
   and thus the conclusion holds.
  Otherwise, Theorem \ref{thm:Fermat} completes the proof.
\end{proof}

\begin{rem}
Theorem \ref{thm:intermediateValue} is about a closed interval
 and Theorem \ref{thm:Fermat} an open interval.
Thus in the proof of Theorem \ref{thm:Rolles}
 we must treat the special case of $f$ being a constant.
\end{rem}

\begin{thm}[Mean value]
  \label{thm:meanValue}
  If $f\in {\cal C}[a,b]$ and if $f'$ exists on $(a,b)$,
  then $\exists \xi\in(a,b)$
  s.t. $f(b)-f(a)=f'(\xi)(b-a)$.
\end{thm}
\begin{proof}
  Construct a linear function
   $L:[a,b]\rightarrow\mathbb{R}$ such that
   $L(a)=f(a)$, $L(b)=f(b)$,
   then $\forall x\in (a,b)$,
   we have $L'(x)=\frac{f(b)-f(a)}{b-a}$.
  Consider $g(x)=f(x)-L(x)$ on $[a,b]$.
   $g(a)=0$, $g(b)=0$.
  By Theorem \ref{thm:Rolles}, $\exists \xi\in[a,b]$
   such that $g'(\xi)=0$,
   which completes the proof.
\end{proof}


\subsection{Taylor series}
\label{sec:Taylorseries}

\begin{defn}
  \label{def:powerSeries}
  A \emph{power series} centered at $c$
   is a series of the form
   \begin{equation}
     \label{eq:powerSeries}
     p(x)=\sum_{n=0}^{\infty} a_n (x-c)^n,
   \end{equation}
   where $a_n$'s are the \emph{coefficients}.
  The \emph{interval of convergence}
   is the set of values of $x$ for which the series converges:
   \begin{equation}
     \label{eq:intervalOfConvergence}
     I_c(p) = \{x\ |\ p(x) \text{ converges} \}.
   \end{equation}
\end{defn}

\begin{defn}
  \label{def:TaylorPolynomial}
  If the derivatives $f^{(i)}(x)$ with $i=1,2,\ldots,n$ exist for a function
  $f: \mathbb{R}\rightarrow \mathbb{R}$ at $x=c$,
   then
   \begin{equation}
     \label{eq:partialSumTaylor}
     T_n(x) = \sum_{k=0}^{n}\frac{f^{(k)}(c)}{k!}(x-c)^k
   \end{equation}
   is called the $n$th \emph{Taylor polynomial} for $f(x)$ at $c$.
\\  In particular, 
the \emph{linear approximation} for $f(x)$ at $c$ is
   \begin{equation}
     \label{eq:linearTaylor}
     T_1(x) = f(c) + f'(c)(x-c).
   \end{equation}
 \end{defn}

 \begin{exm}
   \label{exm:TaylorPoly}
  If $f\in {\cal C}^{\infty}$, then $\forall n\in\mathbb{N}$,
   we have
  \begin{equation*}
    T_n^{(m)}(x) = \left\{
    \begin{array}{ll}
      \sum_{k=m}^n\frac{f^{(k)}(c)}{(k-m)!}(x-c)^{k-m}, &
      m\in\mathbb{N}, m\le n;
      \\
      0, & m\in\mathbb{N}, m> n.
    \end{array}
    \right.
  \end{equation*}
This can be proved by induction.
In the inductive step, we regroup the summation
 into a constant term and another shifted summation.
\end{exm}

\begin{defn}
  \label{def:TaylorSeries}
The \emph{Taylor series} (or Taylor expansion) 
   for $f(x)$ at $c$ is
 \begin{equation}
     \label{eq:TaylorSeries}
     %\lim_{n\rightarrow\infty}T_n(x)= 
     \sum_{k=0}^{\infty}\frac{f^{(k)}(c)}{k!}(x-c)^k.
   \end{equation}
\end{defn}

\begin{defn}
The \emph{remainder} of the $n$th \emph{Taylor polynomial}
 in approximating $f(x)$ is
  \begin{equation}
    \label{eq:TaylorSeriesRemainder}
    E_n(x) = f(x) - T_n(x).
  \end{equation}
\end{defn}

\begin{thm}
  \label{thm:limitTaylorRemainder}
  Let $T_n$ be the $n$th Taylor polynomial for $f(x)$ at $c$.
  \begin{equation}
    \lim_{n\rightarrow\infty} E_n(x) = 0
    \ \Leftrightarrow\
%    f(x) = \sum_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}(x-a)^k
    \lim_{n\rightarrow\infty}T_n(x) = f(x).
  \end{equation}
\end{thm}

\begin{lem}
  \label{lem:remainder}
  $\forall m=0,1,2,\ldots,n$, $E_n^{(m)}(c)=0$.
\end{lem}
\begin{proof}
  This follows from Definition \ref{def:TaylorPolynomial}
   and Example \ref{exm:TaylorPoly}.
\end{proof}

 \begin{thm}[Taylor's theorem with Lagrangian form]
   \label{thm:TaylorLagrangianForm}
   Consider a function
    $f: \mathbb{R} \rightarrow \mathbb{R}$.
   If $f\in {\cal C}^n[c-d,c+d]$
    and $f^{(n+1)}(x)$ exists on $(c-d, c+d)$,
   then
    $\forall x\in [c-d,c+d]$,
    there exists some $\xi$ between $c$ and $x$
    such that
    \begin{equation}
      \label{eq:TaylorTheorem}
      E_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}(x-c)^{n+1}.
    \end{equation}
 \end{thm}
 \begin{proof}
   Fix $x\ne c$,
    let $M$ be the unique solution of 
   \begin{equation*}
%     \label{eq:Mequality}
     E_n(x) = f(x) - T_n(x) = \frac{M(x-c)^{n+1}}{(n+1)!}.
   \end{equation*}
   Consider the function
   \begin{equation}
     \label{eq:TaylorLagrangianFormProof1}
     g(t) := E_n(t) - \frac{M(t-c)^{n+1}}{(n+1)!}.
   \end{equation}
   Clearly $g(x)=0$.
   By Lemma \ref{lem:remainder},
    $g^{(k)}(c)=0$ for each $k= 0,1,\ldots,n$.
   Then Rolle's theorem implies that
   \begin{equation*}
    \exists x_1\in (c, x) \text{ s.t. } g'(x_1)=0. 
   \end{equation*}
   If $x<c$, change $(c,x)$ above to $(x,c)$.
   Apply Rolle's theorem to $g'(t)$ on $(c,x_1)$
   and we have
   \begin{equation*}
    \exists x_2\in (c, x_1) \text{ s.t. } g^{(2)}(x_2)=0. 
   \end{equation*}
   Repeatedly using Rolle's theorem,
   \begin{equation}
     \label{eq:TaylorLagrangianFormProof2}
    \exists x_{n+1}\in (c, x_n) \text{ s.t. } g^{(n+1)}(x_{n+1})=0.
  \end{equation}
  Since $T_n$ is a polynomial of degree $n$,
   we have $T_n^{(n+1)}(t)=0$, 
   which, together with
   (\ref{eq:TaylorLagrangianFormProof2})
   and (\ref{eq:TaylorLagrangianFormProof1}),
   yields 
   \begin{equation*}
     f^{(n+1)}(x_{n+1})-M=0.
   \end{equation*}
   The proof is completed
    by identifying $\xi$ with $x_{n+1}$.
\end{proof}

\begin{exm}
  \label{exm:approximatingExponentialFunction}
  How many terms are needed to compute $e^2$ correctly to four decimal
  places?

  The requirement of four decimal places means
  an accuracy of at least $\epsilon=10^{-5}$.
By Definition \ref{def:TaylorSeries}, 
   the Taylor series of $e^x$ at $c=0$ is
  \begin{equation*}
    e^x = \sum_{n=0}^{+\infty}\frac{x^n}{n!}.
  \end{equation*}
  By Theorem \ref{thm:TaylorLagrangianForm}, we have
  \begin{equation*}
    \exists \xi\in [0,2] \text{ s.t. }
    E_n(2)=e^{\xi}2^{n+1}/(n+1)! < e^22^{n+1}/(n+1)!
  \end{equation*}
  Then $e^22^{n+1}/(n+1)!\le \epsilon$ yields $n\ge 12$, i.e., 13 terms.
\end{exm}


\subsection{Riemann integral}
\label{sec:RiemannIntegral}

\begin{defn}
  \label{def:partitionOfInterval}
  A \emph{partition of an interval} $I=[a, b]$
  is a totally-ordered finite subset $P_n\subseteq I$ of the form
  %a finite sequence of numbers of the form
  \begin{equation}
    \label{eq:partition}
    P_n(a,b) = \{a=x_0 < x_1 < \cdots < x_n=b\}.
  \end{equation}
  The interval $I_i=[x_{i-1}, x_{i}]$ is the $i$th
   \emph{subinterval} of the partition.
%  The \emph{mesh} or 
  The \emph{norm} of the partition
  is the length of the longest subinterval,
  \begin{equation}
    h_n= h(P_n) = \max(x_i-x_{i-1}),\qquad i=1, 2, \ldots, n.
  \end{equation}
\end{defn}

\begin{defn}
  \label{def:RiemannSum}
  The \emph{Riemann sum} of %a function
  $f: \mathbb{R}\rightarrow \mathbb{R}$
  over a partition $P_n$ is
  \begin{equation}
    \label{eq:RiemannSum}
    S_n(f) = \sum_{i=1}^n f(x_i^*) (x_{i}-x_{i-1}),
  \end{equation}
  where $x_i^*\in I_i$ 
%  $x_i^*\in [x_{i-1}, x_i]$ 
  is a \emph{sample point} % or a \emph{tag}
  of the $i$th subinterval.
  In particular, the \emph{upper Riemann sum}
  and \emph{lower Riemann sum} over $P_n$ are
  \begin{align}
    \label{eq:upperRiemannSum}
    M_n(f) = \sum_{i=1}^n (x_{i}-x_{i-1}) \sup f(I_i), 
    \\
    \label{eq:lowerRiemannSum}
    m_n(f) = \sum_{i=1}^n (x_{i}-x_{i-1}) \inf f(I_i). 
  \end{align}
\end{defn}

\begin{defn}
  \label{def:RiemannIntegrable}
  A function $f: \mathbb{R}\rightarrow \mathbb{R}$
  is  %\emph{integrable} (or more precisely
  \emph{Riemann integrable} on  $[a, b]$
  iff %there exists $L\in \mathbb{R}$ such that
% the limit of its Riemann sum exists as $n\rightarrow \infty$
  \begin{align}
    \nonumber
    &\exists L\in\mathbb{R}, \text{ s.t. }
      \forall \epsilon>0,\ \exists \delta>0 \text{ s.t. }
    \\ \label{eq:RiemannIntegrable}
    & \forall P_n(a,b) \text{ with } h(P_n)<\delta,\ 
      |S_n(f)-L|<\epsilon.
  \end{align}
  In this case we write $L=\int_a^b f(x) \dif x$
  and call it the \emph{Riemann integral} of $f$ on $[a,b]$.
\end{defn}

  % Bind each number to the picture that illustrates
  % Riemann integral.

\begin{rem}
  A logical statement is either true or false.
  As one way to prove it,
   imagine that you are debating with someone
   that believes it is false.
  Your opponent will focus on the $\forall$ variables,
   i.e. the variables quantified by the
   $\forall$ quantifiers, 
   and try to instantiate some combination of their values 
   so that the formula fails.
  Your job is to focus on the $\exists$ variables
   so that for any possible combination value of the $\forall$ variables
   you can name a corresponding combination value
   for the $\exists$ variables so that the formula holds.
  From this viewpoint, a proof is simply a formula
   for generating values of the ``$\exists$'' variables
   according to the ``$\forall$'' variables.

  To show a logical statement is false,
   we could first negate the logical statement
   and then use the aforementioned method to show the negation holds.
\end{rem}

\begin{exm}
  The following function $f:[a,b]\rightarrow \mathbb{R}$
   is not Riemann integrable.
   \begin{equation}
     \label{eq:rationalVsIrr}
     f(x)=
     \begin{cases}
       1 & \text{if $x$ is rational};
       \\
       0 & \text{if $x$ is irrational}.
     \end{cases}
   \end{equation}

  To see this, we first
   negate the logical statement in (\ref{eq:RiemannIntegrable})
   to get
  \begin{align*}
    &\forall L\in\mathbb{R},
      \exists \epsilon>0, \text{ s.t. } \forall \delta>0 
    \\
    & \exists P_n(a,b) \text{ with } h(P_n)<\delta,\ 
      \text{ s.t. } |S_n(f)-L|\ge \epsilon.
  \end{align*}
  
  If $|L|< \frac{b-a}{2}$,
  we choose all $x_i^*$'s to be rational
  so that $f(x_i^*)\equiv 1$;
  then (\ref{eq:RiemannSum}) yields $S_n(f)=b-a$.
  For $\epsilon=\frac{b-a}{4}$,
   the formula $|S_n(f)-L|\ge \epsilon$ clearly holds.
  
  If $|L|\ge \frac{b-a}{2}$,
  we choose all $x_i^*$'s to be irrational
  so that $f(x_i^*)\equiv 0$;
  then (\ref{eq:RiemannSum}) yields $S_n(f)=0$.
  For $\epsilon=\frac{b-a}{4}$,
   the formula $|S_n(f)-L|\ge \epsilon$ clearly holds.
\end{exm}

\begin{rem}
  The difficulty in proving many statements of the $\epsilon$-$\delta$ type
  lies in how to classify values of the $\forall$ variables
  for instantiating values of the $\exists$ variables.
\end{rem}

\begin{defn}
  \label{def:definiteIntegral}
  If $f: \mathbb{R}\rightarrow \mathbb{R}$
   is integrable on $[a, b]$,
   then the limit of the Riemann sum of $f$
   is called the \emph{definite integral}
   of $f$ on $[a, b]$:
%, written,
   \begin{equation}
     \label{eq:definiteIntegral}
     \int_a^b f(x)\dif x
     = \lim_{h_n\rightarrow 0} S_n(f).
%\sum_{i=1}^n f(x_i^*) (x_{i}-x_{i-1}).
   \end{equation}
\end{defn}

\begin{thm}
  \label{thm:upperLowerRiemannIntegrals}
  A bounded function $f:[a,b]\rightarrow\mathbb{R}$
  is Riemann integrable if and only if
  the upper and lower Riemann integrals of $f$ are equal: 
  \begin{equation}
    \label{eq:upperLowerRiemannIntegrals}
    \overline{\int}_a^b f \dif x :=
    \inf_{h_n\rightarrow 0} M_n(f), \quad
%    \sum_{i=1}^n(x_i-x_{i-1}) \sup f(I_i),
    \underline{\int}_a^b f \dif x :=
    \sup_{h_n\rightarrow 0} m_n(f), 
%    \sum_{i=1}^n(x_i-x_{i-1}) \inf f(I_i),
  \end{equation}
  where $M_n(f)$ and $m_n(f)$ are respectively defined in
  (\ref{eq:upperRiemannSum}) and (\ref{eq:lowerRiemannSum}). 
\end{thm}

\begin{thm}
  A scalar function $f$ is integrable on $[a, b]$
   if $f\in {\cal C}[a,b]$.
\end{thm}

\begin{defn}
  \label{def:monotonicFunctions}
  A \emph{monotonic} function is a function between ordered sets
   that either preserves or reverses the given order.
  In particular,
   $f:\mathbb{R}\rightarrow\mathbb{R}$
   is \emph{monotonically increasing}
   if $\forall x,y$, $x \le y \Rightarrow f(x)\le f(y)$;
   $f:\mathbb{R}\rightarrow\mathbb{R}$
   is \emph{monotonically decreasing}
   if $\forall x,y$, $x \le y \Rightarrow f(x)\ge f(y)$.
\end{defn}

\begin{rem}
  The ordered sets in Definition \ref{def:monotonicFunctions}
  can be posets,
  but we will limit our attention to chains.
\end{rem}

\begin{thm}
  A scalar function is integrable on $[a, b]$
   if it is monotonic on $[a, b]$.
\end{thm}

\begin{exc}
  True or false:
   a bijective function is either order-preserving
    or order-reversing?
\end{exc}
\begin{solution}
  False; missing continuity.
  In other words,
   a continuous bijective function is either order-preserving
    or order-reversing.
\end{solution}

\begin{thm}[Integral mean value]
  \label{thm:integralMeanValue}
  Let $w:[a,b]\rightarrow \mathbb{R}^+$
  be integrable on $[a,b]$. For $f\in {\cal C}[a,b]$,
  $\exists \xi\in[a,b]$ s.t.
  \begin{equation}
    \label{eq:integralMeanValue}
    \int_a^b w(x)f(x)\dif x = f(\xi) \int_a^b w(x)\dif x.
  \end{equation}
\end{thm}
\begin{proof}
%    see page 19 on the text.
  Denote $m=\inf_{x\in[a,b]} f(x)$, $M=\sup_{x\in[a,b]} f(x)$,
   and $I=\int_a^b w(x)\dif x$.
  Then $m w(x)\le f(x)w(x) \le M w(x)$ and
  \begin{equation*}
    m I \le \int_a^b w(x) f(x) \dif x \le M I.
  \end{equation*}
  $w>0$ implies $I\ne 0$, hence
  \begin{equation*}
    m \le \frac{1}{I}\int_a^b w(x) f(x) \dif x \le M.
  \end{equation*}
  Applying Theorem \ref{thm:intermediateValue}
   completes the proof.
\end{proof}

\begin{thm}[First fundamental theorem of calculus]
  \label{thm:fundamentalThmCalculus1}
  Let $a<b$ be real numbers.
  For a continuous function $f: [a,b]\rightarrow \mathbb{R}$
  that is Riemann integrable, 
  define a function $F: [a,b]\rightarrow \mathbb{R}$ by
  \begin{equation}
    \label{eq:integralOfRiemannIntF}
    F(x) := \int_a^x f(y) \dif y.
  \end{equation}
  Then $F$ is differentiable and
  \begin{equation}
    \label{eq:fundamentalThmCalculus1}
    \forall x_0\in [a,b], \quad
    F'(x_0) = f(x_0).
  \end{equation}
\end{thm}

\begin{thm}[Second fundamental theorem of calculus]
  \label{thm:fundamentalThmCalculus2}
  Let $a<b$ be real numbers
  and let $f: [a,b]\rightarrow \mathbb{R}$
  be a Riemann integrable function.
  If $F:[a,b]\rightarrow\mathbb{R}$ is the antiderivative of $f$,
  i.e. $F'(x)=f(x)$,
  then
  \begin{equation}
    \label{eq:fundamentalThmCalculus2}
    \int_a^b f = F(b) - F(a).
  \end{equation}
\end{thm}


\section{Vector functions
  $\mathbb{R}^m\rightarrow\mathbb{R}^n$}
\label{sec:sever-vari-diff}

\begin{rem} 
  % In Definition \ref{def:MatrixNorms},
  % we used a vector norm to define a matrix norm.
  % This is a typical example
  % of connecting new stuff 
  % to things we already know.
  In Definition \ref{def:norm},
  the concept of norms are defined with respect
  to a vector space ${\cal V}$,
  which is either $\mathbb{R}^n$ or $\mathbb{C}^n$
  in lower-level classes such as linear algebra,
  but ${\cal V}$ could be a vector space of functions
  in upper-level classes such as
  functional analysis.
  To transit smoothly from linear algebra
  to functional analysis,
  in Sections \ref{sec:operator-norm}
  we rephrase the induced matrix 2-norm
  as an operator norm.
  A careful study of this operator norm
  and Hilber-Schmidt norm
  serves as a stepping stone
  to the more abstract discussion of norms
  in Sections \ref{sec:norm-equivalence}
  and \ref{sec:space-CLXY}.
  Then the norm equivalence is utilized
  to define the matrix exponential
  in Sections \ref{sec:matrix-exponential}.
  % Most contents in the first two subsections
  % are taken from \cite{knapp05:_basic_real_analy}.
\end{rem}

\begin{ntn}
  \label{ntn:lengthOfVec}
  The \emph{length of a Euclidean vector}
  $\mathbf{x}\in \mathbb{F}^n$
  where $\mathbb{F}=\mathbb{R}$ or $\mathbb{F}=\mathbb{C}$
  is denoted by $|\mathbf{x}|:=\|\mathbf{x}\|_2$
  where $\|\cdot\|_2$ is
  the Euclidean 2-norm in (\ref{eq:EuclideanLpNorm}).
\end{ntn}

\begin{rem}
  Notation \ref{ntn:lengthOfVec} is a natural generalization
  of the magnitude of a real number and the modulus of a complex number;
  it will be further generalized to a matrix as its Hilbert-Schmidt norm
  in Definition \ref{def:HilbertSchmidtNorm}.
\end{rem}


\subsection{Operator norm and Frobenius norm}
\label{sec:operator-norm}

\begin{lem}
  \label{lem:linearOperatorBounded}
  The length-scaling factor of a linear map
  \mbox{$T\in {\cal L}(\mathbb{F}^n,\mathbb{F}^m)$}
  upon any vector is bounded, i.e., 
  % the vector space of all linear maps from $\mathbb{F}^n$
  % to $\mathbb{F}^m$.
  \begin{equation}
    \label{eq:linearOperatorBounded}
    \forall T\in {\cal L}(\mathbb{F}^n,\mathbb{F}^m),\ 
    \exists M\in \mathbb{F} \text{ s.t. }
    \forall \mathbf{x}\in \mathbb{F}^n,\
    |T\mathbf{x}|\le M|\mathbf{x}|,
  \end{equation}
\end{lem}
\begin{proof}
  Since $\mathbf{x}=\sum_j x_j\mathbf{e}_j$
  and $T$ is a linear map, we have
  \begin{displaymath}
    \begin{array}{rl}
    |T\mathbf{x}| &= \left|\sum_j x_j T \mathbf{e}_j\right|
                    \le \sum_j \left| x_j T \mathbf{e}_j\right|
                    = \sum_j \left| x_j||T \mathbf{e}_j\right|
    \\
    &\le  |\mathbf{x}| \sum_j\left|T \mathbf{e}_j\right|, 
    \end{array}
  \end{displaymath}
  where the second step follows from the triangle inequality
  in Definition \ref{def:norm}, 
  the third step from the absolute homogeneity 
  in Definition \ref{def:norm}, 
  and the last step from $|x_j|\le |\mathbf{x}|$.
  The proof is completed by setting
  $M:=\sum_j\left|T \mathbf{e}_j\right|$.
\end{proof}

\begin{defn}
  \label{def:uniformlyContinuousVector}
  A function $f: \mathbb{F}^m\rightarrow \mathbb{F}^n$
  is \emph{uniformly continuous} in $U\subset \mathbb{F}^m$
  iff
   \begin{equation}
     \label{eq:uniformlyContinuous}
     \begin{array}{l}
     \forall \epsilon>0, \exists \delta>0,\text{ s.t. }
     \\
     \forall x,y\in I,\ 
       |\mathbf{x}-\mathbf{y}|<\delta \Rightarrow
       |\mathbf{f}(\mathbf{x})-\mathbf{f}(\mathbf{y})|<\epsilon.
     \end{array}
   \end{equation}
\end{defn}

\begin{rem}
  Definition \ref{def:uniformlyContinuousVector}
  is a straightforward generalization
  of Definition \ref{def:uniformlyContinuousScalar}.
  One main point of this subsection is to
  equip us with uniform continuity of a linear map
  so that we can discuss its size or length, c.f. Notation \ref{ntn:lengthOfVec}.
\end{rem}

\begin{coro}
  \label{coro:linearMapIsUniformlyCont}
  Any linear map
  $T\in {\cal L}(\mathbb{F}^n,\mathbb{F}^m)$
  is uniformly continuous on $\mathbb{F}^n$.
\end{coro}
\begin{proof}
  For any $\mathbf{x},\mathbf{y}\in \mathbb{F}^n$,
  we have
  \begin{displaymath}
    |T\mathbf{x} - T\mathbf{y}|
    = |T(\mathbf{x} - \mathbf{y})|
    \le M |\mathbf{x} - \mathbf{y}|,
  \end{displaymath}
  where the second step follows
  from Lemma \ref{lem:linearOperatorBounded}. 
  The rest follows from setting $\delta = \frac{\epsilon}{M}$
  in Definition \ref{def:uniformlyContinuousVector}.
\end{proof}

\begin{defn}
  \label{def:operatorNorm}
  The \emph{operator norm} of a linear map
  \mbox{$T\in {\cal L}(\mathbb{F}^n,\mathbb{F}^m)$}
  is the non-negative number %in $\mathbb{F}$,
  \begin{equation}
    \label{eq:operatorNorm}
    \|T\| := \inf%_{\mathbf{x}\in \mathbb{F}^n}
    \left\{
      M\ge 0 : \forall \mathbf{x}\in \mathbb{F}^n,
      |T\mathbf{x}| \le M |\mathbf{x}|
    \right\}.
  \end{equation}
\end{defn}

\begin{coro}
  \label{coro:2normLessThanOpNorm}
  $\forall \mathbf{x}\in \mathbb{F}^n,\
  |T\mathbf{x}| \le \|T\||\mathbf{x}|$.
\end{coro}
\begin{proof}
  For any given $T$,
   Lemma \ref{lem:linearOperatorBounded} implies that
   the set
   \begin{displaymath}
     {\cal M}:=\left\{ M\ge 0 : \forall \mathbf{x}\in \mathbb{F}^n,
       |T\mathbf{x}| \le M |\mathbf{x}| \right\}
   \end{displaymath}
  is nonempty and bounded from below.
%  It is also closed: 
  For any Cauchy sequence $\{M_n\}_{n=1}^{\infty}$ in ${\cal M}$
  that converges to $c$,
  we can take the limit to the sequence of inequalities
  $|T\mathbf{x}|\le M_n |\mathbf{x}|$
  to obtain $|T\mathbf{x}|\le c |\mathbf{x}|$.
  Hence $c\in {\cal M}$ and ${\cal M}$ is closed.
  It follows that the infimum of ${\cal M}$
  is contained in ${\cal M}$.
\end{proof}

\begin{rem}
  By Lemma \ref{lem:spectralRadiusLessEqNorm},
  we have $\rho(A)\le \|A\|$.
  The equality holds when $A$ is a normal matrix.
  For a nilpotent matrix such as a Jordan block $J$
  with $\lambda=0$,
  we have $\rho(J)=0$
  but $\|J\|>0$.
\end{rem}

\begin{exc}
  Verify that (\ref{eq:operatorNorm})
  is indeed a norm in the sense of Definition \ref{def:norm}.
\end{exc}
\begin{solution}
  For the point separation, we have
  \begin{displaymath}
    \begin{array}{rl}
    \|T\| = 0 & \implies \inf\{M: \forall \mathbf{x} \in
                \mathbb{F}^n, |T\mathbf{x}| \leq M|\mathbf{x}|\} =0
      \\ &\implies \forall \mathbf{x} \in \mathbb{F}^n,
           |T\mathbf{x}| \leq 0
      \\ &\implies \forall \mathbf{x} \in \mathbb{F}^n, |T\mathbf{x}| = 0 \implies T = 0,
    \end{array}
  \end{displaymath}
  where the second step follows from
  taking the limit to a sequence of equalities
  $|T\mathbf{x}|\le M_n |\mathbf{x}|$
  with the sequence $\{M_n\}_{n=1}^{\infty}$
  satisfying $\lim_{n\rightarrow +\infty} M_n = 0$.

  The absolute homogeneity holds because $\forall a \in \mathbb{F}$, 
  \begin{displaymath}
    \begin{array}{rl}
%      &\forall T \in \mathcal{L}(\mathbb{F}^n,\mathbb{F}^m),
      \|aT\| &= \inf\{M:\forall \mathbf{x} \in \mathbb{F}^n,|aT
               \mathbf{x} | \leq M|\mathbf{x}|\}
      \\ &= \inf\{M:\forall \mathbf{x} \in
           \mathbb{F}^n,|a||T\mathbf{x}| \leq M|\mathbf{x}|\}
      \\ &= |a|\inf\{M:\forall \mathbf{x} \in
           \mathbb{F}^n,|T\mathbf{x}| \leq M|\mathbf{x}|\}
      \\ &= |a|\|T\|.
    \end{array}
  \end{displaymath}
  The triangle inequality holds because
  \begin{displaymath}
    \begin{array}{rl}
      % & \forall T, S \in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^m),
      % \\
      \|T + S\| &= \inf\{M: \forall \mathbf{x} \in \mathbb{F}^n,
                  |(T+S)\mathbf{x}| \leq M|\mathbf{x}|\}\\
      &= \inf\{M:\forall \mathbf{x} \in \mathbb{F}^n, |T\mathbf{x}+S\mathbf{x}| \leq M|\mathbf{x}|\} \\
      &\leq \inf\{M:\forall \mathbf{x} \in \mathbb{F}^n, |T\mathbf{x}|+|S\mathbf{x}| \leq M|\mathbf{x}|\}\\
      &\leq \inf\{M:\forall \mathbf{x} \in \mathbb{F}^n,
        |T\mathbf{x}| \leq M|\mathbf{x}|\} \\
      &\ \  + \inf\{M:\forall \mathbf{x} \in \mathbb{F}^n, |S\mathbf{x}| \leq M|\mathbf{x}|\}\\
      &= \|T\| + \|S\|.
    \end{array}
  \end{displaymath}
  Finally the real positivity holds trivially.
\end{solution}

\begin{coro}
  \label{coro:operatorNormAsSupOf2norm}
  $\|T\|=\sup_{|\mathbf{x}|\le 1} |T \mathbf{x}|
  = \sup_{|\mathbf{x}|=1} |T\mathbf{x}|$.
\end{coro}
\begin{proof}
  Since $T$ is a linear map
  and $|\cdot|$ is a norm,
  we have
  \begin{displaymath}
    |T(c\mathbf{x})|=|cT\mathbf{x}|=|c||T\mathbf{x}|.
  \end{displaymath}
  Hence the inequality $|T\mathbf{x}| \le M |\mathbf{x}|$
  in (\ref{eq:operatorNorm}) holds
  for all $\mathbf{x}\ne 0$
  if and only if it holds for all $\mathbf{x}$
  with $|\mathbf{x}|\in (0,1]$, 
  if and only if it holds for all $\mathbf{x}$
  with $|\mathbf{x}|=1$.
\end{proof}

\begin{rem}
  In some books, Corollary \ref{coro:operatorNormAsSupOf2norm}
  is used to define the operator norm
  \emph{induced} from the Euclidean 2-norm.
  Indeed, Corollary \ref{coro:operatorNormAsSupOf2norm}
  and (\ref{eq:operatorNorm})
  are equivalent definitions of this operator norm.
  It is emphasized that the operator norm
  depends on the choice of the vector norm.
\end{rem}

\begin{coro}
  \label{coro:compositionTriangleIneq}
  The composition of two linear maps
  \mbox{$S\in {\cal L}(\mathbb{F}^n,\mathbb{F}^m)$}
   and \mbox{$T\in {\cal L}(\mathbb{F}^m,\mathbb{F}^k)$}
  satisfies
  \begin{equation}
    \label{eq:compositionTriangleIneq}
    % T\in {\cal L}(\mathbb{F}^n,\mathbb{F}^m),\
    % S\in {\cal L}(\mathbb{F}^m,\mathbb{F}^k)\
    % \Rightarrow\ 
    \|TS\| \le \|T\| \|S\|.
  \end{equation}
\end{coro}
\begin{proof}
  By Corollary \ref{coro:2normLessThanOpNorm}, we have
  \begin{displaymath}
    |(TS)(\mathbf{x})| = |T(S\mathbf{x})|
    \le \|T\||S\mathbf{x}| \le \|T\|\|S\||\mathbf{x}|.
  \end{displaymath}
  Taking supremum of the above for $|\mathbf{x}|\le 1$
  and applying Corollary \ref{coro:operatorNormAsSupOf2norm}
  yield (\ref{eq:compositionTriangleIneq}).
\end{proof}

\begin{coro}
  The identity function $I\in {\cal L}(\mathbb{F}^n)$
  satisfies $\|I\|=1$.
\end{coro}
\begin{proof}
  This follows directly from (\ref{eq:operatorNorm}).
\end{proof}

\begin{exc}
  Verify that the space ${\cal L}(\mathbb{F}^n,\mathbb{F}^m)$
  becomes a metric space
  if we define the metric as $d(T,S)=\|T-S\|$.
\end{exc}
\begin{solution}
  We verify the conditions of a metric space.
  \begin{itemize}\itemsep0em
  \item[(i)] $\forall T, S \in \mathcal{L}(\mathbb{F}^n,
    \mathbb{F}^m),
    d(T, S) = \|T - S\| \geq 0 $
    and $d(T, S) = 0 \iff \|T - S\| = 0 \iff T - S = 0 \iff T = s$;
  \item[(ii)] $d(T, S) = \|T - S\| = \|S - T\| = d(S, T)$;
  \item[(iii)] $\forall T, S ,V \in \mathcal{L}(\mathbb{F}^n,
    \mathbb{F}^m),
    d(T, V) = \|T - V\| = \|T - S + S - V\| \leq \|T - S\| + \|S - V\|
    = d(T, S) + d(S, V)$.
  \end{itemize}
\end{solution}

\begin{rem}
  If an operator $T\in{\cal L}(\mathbb{C}^n,\mathbb{C}^m)$
  can also be considered as another operator
  $\mathbb{R}^n \rightarrow \mathbb{R}^m$, 
  there is a potential glitch of Definition \ref{def:operatorNorm}
  that the operator norms
  of these two operators are different.
  The following lemma prevents this potential glitch
  from happening.
\end{rem}

\begin{lem}
  \label{lem:consistencyNestedRealInsideComplex}
  For $T\in{\cal L}(\mathbb{C}^n,\mathbb{C}^m)$, 
  suppose for each standard basis vectors $\mathbf{e}_j$
  in Definition \ref{def:standardBasis}
  we have $T \mathbf{e}_j \in \mathbb{R}^m$.
  Then $T$ carries $\mathbb{R}^n$ into $\mathbb{R}^m$
  and $\|T\|$ is consistently defined in the sense that
  \begin{equation}
    \label{eq:consistencyNestedRealInsideComplex}
    \|T\|
    = \sup_{\mathbf{x}\in \mathbb{R}^n; |\mathbf{x}|\le 1} |T \mathbf{x}|
    = \sup_{\mathbf{z}\in \mathbb{C}^n; |\mathbf{z}|\le 1} |T \mathbf{z}|.
  \end{equation}
\end{lem}
\begin{proof}
  First, $T(\mathbb{R}^n)$ $\subset \mathbb{R}^m$ is trivial because the matrix of $T$ is a real matrix.
  Thus, $T \in \mathcal{L}(\mathbb{R}^n, \mathbb{R}^m)$. From this fact, we can define an operator norm of $T$ as
  \begin{equation}\label{equ1}
    \|T\| = \underset{\mathbf{x}\in \mathbb{R}^n;|\mathbf{x}| \leq 1}{\sup}|T\mathbf{x}|.
  \end{equation}
  On the one hand, we have
  \begin{equation}\nonumber
    \underset{\mathbf{x}\in \mathbb{R}^n;|\mathbf{x}| \leq 1}{\sup}|T\mathbf{x}| \leq \underset{\mathbf{z}\in \mathbb{C}^n;|\mathbf{z}| \leq 1}{\sup}|T\mathbf{z}|
  \end{equation}
  since $\mathbb{R}^n \subset \mathbb{C}^n$. 
  On the other hand,
  $\forall \mathbf{z} \in \mathbb{C}^n$,
  $|\mathbf{z}| \leq 1$,
  $\exists \mathbf{x},\mathbf{y} \in \mathbb{R}^n$,
  s.t. $\mathbf{z} = \mathbf{x} + \ii \mathbf{y}$
  and $|\mathbf{x}|^2 + |\mathbf{y}|^2 \leq 1$, and we have
  \begin{align*}
    |T\mathbf{z}| &= |T\mathbf{x} + \ii T\mathbf{y}| =
                    \sqrt{|T\mathbf{x}|^2 + |T\mathbf{y}|^2}
    \\
                  &\le \sqrt{\|T\|^2|\mathbf{x}|^2 + \|T\|^2|\mathbf{y}|^2}
    \\
    &= \|T\|\sqrt{|\mathbf{x}|^2 + |\mathbf{y}|^2} \leq \|T\| = \underset{\mathbf{x}\in \mathbb{R}^n;|\mathbf{x}| \leq 1}{\sup}|T\mathbf{x}|  
  \end{align*}
  \noindent where the third step follows from Corollary
  \ref{coro:2normLessThanOpNorm}, 
  the fifth step from $|\mathbf{x}|^2 + |\mathbf{y}|^2 \leq 1$, and
  the last step from (\ref{equ1}). Thus we have
  \begin{displaymath}
    \underset{\mathbf{x}\in \mathbb{R}^n;
      |\mathbf{x}| \leq 1}{\sup}|T\mathbf{x}| \geq
    \underset{\mathbf{z}\in \mathbb{C}^n;
      |\mathbf{z}| \leq 1}{\sup}|T\mathbf{z}|,
  \end{displaymath}
  which completes the proof.
\end{proof}

\begin{defn}
  \label{def:HilbertSchmidtNorm}
  The \emph{Hilbert-Schmidt norm} or the \emph{Frobenius norm}
  of a linear map
  \mbox{$T\in {\cal L}(\mathbb{F}^n,\mathbb{F}^m)$}
  is the non-negative number 
  \begin{equation}
    \label{eq:HilbertSchmidtNorm}
    |T| := \left(
      \sum_{j=1}^n \left|T \mathbf{e}_j\right|^2
    \right)^{\frac{1}{2}}
  \end{equation}
  where $\mathbf{e}_1, \ldots, \mathbf{e}_n$
  is the standard basis %of $\mathbb{F}^n$
  in Definition \ref{def:standardBasis}.
\end{defn}

\begin{exc}
  Verify that (\ref{eq:HilbertSchmidtNorm})
  is indeed a norm in the sense of Definition \ref{def:norm}.
\end{exc}
\begin{solution}
  The real positivity holds trivially.
  For the point separation, we have
  $|T| = 0 \implies \forall 1 \leq j \leq n,|T\mathbf{e}_j|^2 = 0
  \implies \forall 1 \leq j \leq n, T\mathbf{e}_j = 0 \implies \forall
  \nu \in \mathbb{F}^n, T\nu = 0 \implies T = 0$.

  The absolute homogeneity holds because
  $\forall a \in \mathbb{F}, \forall T \in \mathcal{L}(\mathbb{F}^n,
  \mathbb{F}^m), |aT| = (\sum\limits_{j =
    1}^n|aT\mathbf{e}_j|^2)^{\frac{1}{2}} = |a|(\sum\limits_{j =
    1}^n|T\mathbf{e}_j|^2)^{\frac{1}{2}} = |a||T|$.

  Finally, the triangle inequality holds because
  $\forall T, S \in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$, we have
    \begin{align*}
      &|T + S|^2 = \sum\limits_{j = 1}^n|(T+S)\mathbf{e}_j|^2
      \\ =& \sum\limits_{j = 1}^n|T\mathbf{e}_j + S\mathbf{e}_j|^2
      \\
      =& \sum\limits_{j = 1}^n|T\mathbf{e}_j|^2
         + 2\sum\limits_{j = 1}^n|(T\mathbf{e}_j)^H(S\mathbf{e}_j)|
         + \sum\limits_{j = 1}^n|S\mathbf{e}_j|^2
      \\
      \le& \sum\limits_{j = 1}^n|T\mathbf{e}_j|^2
           + 2\sum\limits_{j = 1}^n|T\mathbf{e}_j||S\mathbf{e}_j|
           + \sum\limits_{j = 1}^n|S\mathbf{e}_j|^2
      \\
      \le& \sum\limits_{j = 1}^n|T\mathbf{e}_j|^2 %+|S\mathbf{e}_j|^2\right)
           + 2\sqrt{\left(\sum\limits_{j = 1}^n|T\mathbf{e}_j|^2\right)
           \sum\limits_{j = 1}^n|S\mathbf{e}_j|^2}
           + \sum\limits_{j = 1}^n|S\mathbf{e}_j|^2
      \\
      =& \left(\sqrt{\sum\limits_{j = 1}^n|T\mathbf{e}_j|^2} +
         \sqrt{\sum\limits_{j = 1}^n|S\mathbf{e}_j|^2}\right)^2 = (|T|
         + |S|)^2, 
    \end{align*}
  where the fifth step follows from Cauchy inequality.  
\end{solution}

\begin{coro}
  \label{coro:HilbertSchmidtNormMatrix}
  The matrix $A$ %of the Hilbert-Schmidt norm
  of \mbox{$T\in {\cal L}(\mathbb{F}^n,\mathbb{F}^m)$}
  satisfies
  \begin{equation}
    \label{eq:HilbertSchmidtNormMatrix}
    |A| = \left(
      \sum_{i,j} |a_{ij}|^2
    \right)^{\frac{1}{2}}
  \end{equation}
\end{coro}
\begin{proof}
  Since $A$ is a linear map, we rewrite (\ref{eq:HilbertSchmidtNorm}) as
  \begin{displaymath}
    |A|^2 =  |A \mathbf{e}_1|^2 + |A \mathbf{e}_2|^2 + \cdots
    + |A \mathbf{e}_n|^2.
  \end{displaymath}
  The proof is completed by the fact that
  $|\cdot|$ is the Euclidean 2-norm.
\end{proof}

\begin{rem}
  The notation $|\cdot|$ for a scalar and a vector
  is consistent in the sense that
  it represents the length of both of them.
  Due to (\ref{eq:HilbertSchmidtNormMatrix}),
  this notation also makes sense for a linear map
  and its matrix.
\end{rem}

\begin{coro}
  \label{coro:HSnormIneq}
  $\forall \mathbf{x}\in \mathbb{F}^n,\
  |T\mathbf{x}| \le |T||\mathbf{x}|$.
\end{coro}
\begin{proof}
  We have
  \begin{displaymath}
    \begin{array}{rl}
    |T \mathbf{x}| 
    &= \left|\sum_j x_j T\mathbf{e}_j\right|
    \le \sum_j |x_j| \left|T\mathbf{e}_j\right|
    \\
    &\le \left(\sum_j |x_j|^2\right)^{\frac{1}{2}}
    \left(\sum_j \left|T\mathbf{e}_j\right|^2
      \right)^{\frac{1}{2}}
      = |T||\mathbf{x}|,
    \end{array}
  \end{displaymath}
  where the first inequality follows from
  (NRM-3,4) in Definition \ref{def:norm}, 
  the second inequality from
  the Cauchy-Schwarz inequality (\ref{eq:CauchySchwarz}), 
  and the last step from Definition \ref{def:HilbertSchmidtNorm}.
\end{proof}

\begin{coro}
  \label{coro:compositionTriangleIneqHS}
  The composition of two linear maps
  \mbox{$S\in {\cal L}(\mathbb{F}^n,\mathbb{F}^m)$}
   and \mbox{$T\in {\cal L}(\mathbb{F}^m,\mathbb{F}^k)$}
  satisfies
  \begin{equation}
    \label{eq:compositionTriangleIneqHS}
    |TS| \le |T| |S|.
  \end{equation}
\end{coro}

\begin{exc}
  Prove Corollary \ref{coro:compositionTriangleIneqHS}.
\end{exc}
\begin{solution}
  Let $\mathbf{e}_i$ be the standard basis of $\mathbb{F}^n$. We have
  \begin{align*}
    |TS|^2 &= \sum\limits_{i = 1}^n|TS\mathbf{e}_i|^2
             \leq \sum\limits_{i = 1}^n|T|^2|S\mathbf{e}_i|^2
    \\
           &= |T|^2\sum\limits_{i = 1}^n|S\mathbf{e}_i|^2
             = |T|^2|S|^2, 
  \end{align*}
  where the first step and the last step follow
  from Definition \ref{def:HilbertSchmidtNorm}
  and the second step from Corollary \ref{coro:HSnormIneq}. 
\end{solution}


\begin{coro}
  The identity function $I\in {\cal L}(\mathbb{F}^n)$
  satisfies $|I|=\sqrt{n}$.
\end{coro}
\begin{proof}
  This follows directly from (\ref{eq:HilbertSchmidtNorm}).
\end{proof}

\begin{thm}
  \label{thm:opNormAndHSnorm}
  The operator norm and the Hilbert-Schmidt norm
  on ${\cal L}(\mathbb{F}^n)$
  are related by
  \begin{equation}
    \label{eq:opNormAndHSnorm}
    \|T\| \le |T| \le \sqrt{n} \|T\|.
  \end{equation}
\end{thm}
\begin{proof}
  Take supremum of Corollary \ref{coro:HSnormIneq},
  apply Corollary \ref{coro:operatorNormAsSupOf2norm},
  and we have $\|T\| \le |T|$.
  Then $|T| \le \sqrt{n} \|T\|$ is given by
  \begin{displaymath}
    |T|^2 = \sum_j |T \mathbf{e}_j|^2
    \le \sum_j \|T\|^2 |\mathbf{e}_j|^2 = n \|T\|^2,
  \end{displaymath}
  where the inequality follows from
  Corollary \ref{coro:2normLessThanOpNorm}.
\end{proof}

\begin{rem}
  For a sequence of matrices $(T_n)$,
  $\lim_{n\rightarrow \infty}|T_n-T|=0$
  implies the convergence of the sequence to $T$.
  As a special case of Theorem \ref{thm:normEquivalenceRn}, 
  Theorem \ref{thm:opNormAndHSnorm} states that 
  the Hilbert-Schmidt norm and the operator norm are equivalent
  in measuring the convergence of matrix sequences.
  Therefore,
  convergence in the operator norm
  also implies \emph{entry-by-entry} convergence
  of matrix sequences.
\end{rem}

% \begin{rem}
%   The operator norms discussed in this subsection
%   are for the continuous linear operators
%   on \emph{finite}-dimensional normed spaces
%   and thus are special cases of the operator norm in \ref{eq:OpNormCLXY}.
%   Nonetheless, most proofs here are also applicable
%   to Section \ref{sec:space-CLXY}.
% \end{rem}

\begin{lem}
  \label{lem:invertibleBfromA}
  For $A,B\in {\cal L}(\mathbb{R}^n)$,
  if $A$ is invertible and $\|B-A\|\cdot\|A^{-1}\|<1$,
  then $B$ is also invertible.
\end{lem}
\begin{proof}
  Write $\alpha := \frac{1}{\|A^{-1}\|}$, $\beta := \|B-A\|$,
  and we have $\alpha>\beta$. 
  Then, $\forall \mathbf{x}\in\mathbb{R}^n$,
  \begin{displaymath}
    \begin{array}{rl}
      \alpha |\mathbf{x}|
      &= \alpha|A^{-1}A \mathbf{x}|
        \le \alpha\|A^{-1}\| |A\mathbf{x}|
        %=|A\mathbf{x}|
        = |(A-B+B)\mathbf{x}|
      \\
      & \le \|B-A\||\mathbf{x}| + |B\mathbf{x}|
        = \beta |\mathbf{x}| + |B\mathbf{x}|, 
    \end{array}
  \end{displaymath}
  which implies $0< (\alpha-\beta)|\mathbf{x}|\le |B\mathbf{x}|$.
  Theorem \ref{thm:injectiveIsNull0} implies that
  $B$ is injective and
  Theorem \ref{thm:finiteDimVecSpaceInjecSurjecInvert}
  further implies that $B$ is invertible.
\end{proof}

\begin{rem}
  By its proof, Lemma \ref{lem:invertibleBfromA} holds not only
  for the operator norm,
  but also for any norm of a linear operator. 
  It will be further generalized to infinite-dimensional spaces
  in Theorem \ref{thm:NeumannSeries}.
\end{rem}


\subsection{Differentiation}

\begin{rem}
  \label{rem:singleVarDiff2MultiVarDiff}
  In single variable calculus,
  the derivative of a function $f$ defined in (\ref{eq:derivative})
  is equivalent to 
  \begin{displaymath}
    f'(x_0)=\lim_{x\rightarrow x_0, x\ne x_0} \frac{f(x)-f(x_0)}{x-x_0}.
  \end{displaymath}
  In the several variable case $f: E\rightarrow \mathbb{R}^m$
  where $E$ is now a subset of $\mathbb{R}^n$,
  the nominator $f(x)-f(x_0)$ is a vector in $\mathbb{R}^m$
  and the denominator $x-x_0$ a vector in $\mathbb{R}^n$.
  It does not make sense to divide the two vectors and
   taking norms of the two vectors would also be pointless.
  Therefore a straightforward generalization
  of the above definition of $f'(x_0)$ would not work.

  The solution is to rewrite the concept of derivative
  in one dimension so that generalization to multiple dimensions
  would be much easier.
  The following lemma is the first step.
\end{rem}

\begin{lem}
  \label{lem:derivativeEquiv}
  For $E\subset \mathbb{R}$, $f: E\rightarrow \mathbb{R}$,
  $x_0\in E$, and $L\in \mathbb{R}$,
  the following two statements are equivalent,
  \begin{enumerate}[(a)]\itemsep0em
  \item $f$ is differentiable at $x_0$ and $f'(x_0)=L$;
  \item $\lim_{x\rightarrow x_0, x\in E\setminus\{x_0\}} \frac{|f(x)-
      f(x_0) - L(x-x_0)|}{|x-x_0|}=0$.
  \end{enumerate}
\end{lem}

\begin{exc}
  Prove Lemma \ref{lem:derivativeEquiv}.
\end{exc}

\begin{rem}
  By Lemma \ref{lem:derivativeEquiv},
  the derivative can be interpreted as the number $L$
  for which $|f(x)- f(x_0)-L(x-x_0)|$ is small when $x$ tends to $x_0$,
  even if we divide it by the very small number $|x-x_0|$.
  In other words, the derivative is the quantity $L$
  that makes the approximation $f(x)-f(x_0) \approx L(x-x_0)$
  very accurate.
\end{rem}

\begin{defn}[Total derivative]
  \label{def:derivativeNdim}
  For $E\subset \mathbb{R}^n$, $f: E\rightarrow \mathbb{R}^m$,
  $x_0\in E$, $f$ is \emph{differentiable at $x_0$ with derivative}
  $L\in{\cal L}(\mathbb{R}^n,\mathbb{R}^m)$
  iff 
  \begin{equation}
    \label{eq:derivativeNdim}
    \lim_{x\rightarrow x_0, x\in E\setminus\{x_0\}}
    \frac{|f(x)- f(x_0) - L(x-x_0)|}{|x-x_0|}=0. 
  \end{equation}
  We write $f'(x_0)=L$
  and call it the \emph{total derivative of $f$}.
\end{defn}

\begin{rem}
  Notice the similarity between (\ref{eq:derivativeNdim})
  and the statement (b) in Lemma \ref{lem:derivativeEquiv}.
  The differences are that the derivative is now a linear
  transformation instead of a real number
  and that the distance is now measured by the 2-norm of a vector
  instead of the absolute value of a real number.  
\end{rem}

\begin{exm}
  For $f: \mathbb{R}^2\rightarrow \mathbb{R}^2$
  and $L: \mathbb{R}^2\rightarrow \mathbb{R}^2$,
  \begin{equation}
    \label{eq:fSqaureComp}
    f(x,y):=(x^2, y^2), \ \ L(x,y):=(2x, 4y), 
  \end{equation}
  we claim that $f$ is differentiable at $(1,2)$
  with \mbox{$f'(x_0)=L$} by computing
  \begin{displaymath}
    \renewcommand{\arraystretch}{1.5}
    \begin{array}{rl}
    &\lim_{\stackrel{(x,y)\rightarrow (1,2)}{(x,y)\ne(1,2)}}
    \frac{|f(x,y)- f(1,2) - L((x,y)-(1,2))|}{|(x,y)-(1,2)|}
      \\
    =&\lim_{\stackrel{(a,b)\rightarrow (0,0)}{(a,b)\ne(0,0)}}
    \frac{|f(1+a,2+b)- f(1,2) - L(a,b)|}{|(a,b)|}
      \\
    =&\lim_{\stackrel{(a,b)\rightarrow (0,0)}{(a,b)\ne(0,0)}}
    \frac{|\left((1+a)^2,(2+b)^2\right)- (1,4) - (2a,4b)|}{|(a,b)|}
      \\
    =&\lim_{\stackrel{(a,b)\rightarrow (0,0)}{(a,b)\ne(0,0)}}
    \frac{|(a^2, b^2)|}{|(a,b)|}
    \le \lim_{\stackrel{(a,b)\rightarrow (0,0)}{(a,b)\ne(0,0)}}
          \left(\frac{|(a^2, 0)|}{|(a,b)|}
          +  \frac{|(0, b^2)|}{|(a,b)|}\right)
      \\
    = &\lim_{\stackrel{(a,b)\rightarrow (0,0)}{(a,b)\ne(0,0)}}
        \sqrt{a^2 + b^2}
    = 0.
    \end{array}
  \end{displaymath}
\end{exm}

\begin{lem}
  \label{lem:uniquenessOfDerivatives}
  Let $E$ be a subset of $\mathbb{R}^n$,
  $f: E\rightarrow \mathbb{R}^m$ a function,
  and $x_0\in E$ an interior point of $E$.
  Suppose $f$ is differentiable at $x_0$ with derivative $L_1$
  and also differentiable at $x_0$ with derivative $L_2$.
  Then $L_1=L_2$.
\end{lem}

\begin{exc}
  Prove Lemma \ref{lem:uniquenessOfDerivatives}.
\end{exc}

\begin{rem}
  Thanks to Lemma \ref{lem:uniquenessOfDerivatives},
  we have \emph{the} derivative of $f$ at an interior point $x_0$.
  As another consequence of Lemma \ref{lem:uniquenessOfDerivatives},
  if $f(x)=g(x)$ for all $x\in E$
  and $f,g$ are both differentiable at an interior point $x_0$ of $E$,
  then $f'(x_0)=g'(x_0)$.
\end{rem}

\begin{defn}[Directional derivative]
  \label{def:directionalDerivative}
  Let $E$ be a subset of $\mathbb{R}^n$,
  $f: E\rightarrow \mathbb{R}^m$ a function,
  $x_0\in E$ an interior point of $E$,
  and $\mathbf{v}\in \mathbb{R}^n$ a vector.
  If the limit
  \begin{displaymath}
    \lim_{t\rightarrow 0; t>0, x_0+t\mathbf{v}\in E} \frac{f(x_0+t\mathbf{v}) - f(x_0)}{t}
  \end{displaymath}
  exists, we say that $f$ is \emph{differentiable in the direction $\mathbf{v}$
    at $x_0$},
  and we denote this limit as
  \begin{equation}
    \label{eq:directionalDerivative}
    D_\mathbf{v} f(x_0) := \lim_{t\rightarrow 0; t>0, x_0+t\mathbf{v}\in E}
    \frac{f(x_0+t\mathbf{v}) - f(x_0)}{t}.
  \end{equation}
\end{defn}

\begin{exm}
  For $\mathbf{v}=(3,4)$ and $f: \mathbb{R}^2\rightarrow\mathbb{R}^2$ defined in
  (\ref{eq:fSqaureComp}),
  we have $D_{\mathbf{v}}f(1,2)=(6,16)$.
\end{exm}

\begin{exm}
  For $f: \mathbb{R}\rightarrow\mathbb{R}$,
  $D_{+1} f(x)$ is the right derivative of $f$ at $x$ (if it exists),
  and similarly $D_{-1}f(x)$ is the left derivative of $f$ at $x$ (if it exists).
\end{exm}

\begin{rem}
  The following lemma connects total derivatives and directional derivatives.
\end{rem}

\begin{lem}
  \label{lem:totalDiffImpliesDirectionalDiff}
  Let $E$ be a subset of $\mathbb{R}^n$,
  $f: E\rightarrow \mathbb{R}^m$ a function,
  $x_0\in E$ an interior point of $E$,
  and $\mathbf{v}\in \mathbb{R}^n$ a vector.
  If $f$ is differentiable at $x_0$,
  then $f$ is also differentiable in the direction $\mathbf{v}$ at $x_0$, and
  \begin{equation}
    \label{eq:totalDiffImpliesDirectionalDiff}
    D_\mathbf{v} f(x_0) = f'(x_0) \mathbf{v}.
  \end{equation}
\end{lem}

\begin{rem}
  The converse of Lemma \ref{lem:totalDiffImpliesDirectionalDiff} is
  false.
\end{rem}

\begin{defn}[Partial derivative]
  \label{def:partialDerivative}
  Let $E$ be a subset of $\mathbb{R}^n$,
  $f: E\rightarrow \mathbb{R}^m$ a function,
  $x_0\in E$ an interior point of $E$,
  and $1\le j \le n$.
  The \emph{partial derivative of $f$ with respect to the $x_j$
    variable at} $x_0$ is defined by
  \begin{equation}
    \label{eq:partialDerivative}
    \begin{array}{ll}
    \frac{\partial f}{\partial x_j}(x_0) &:=
    \lim_{t\rightarrow 0; t>0, x_0+te_j\in E} \frac{f(x_0+te_j) -
      f(x_0)}{t} \\
      &= \frac{\dif}{\dif t} \left. f(x_0+te_j)\right|_{t=0}
    \end{array}
  \end{equation}
  provided that the limit exists.
  Here $e_j$ is the $j$th standard basis vector of $\mathbb{R}^n$.
\end{defn}

\begin{defn}
  \label{def:gradient}
  The \emph{gradient} of a differentiable function
  $f: \mathbb{R}^n\rightarrow \mathbb{R}$ is
  \begin{equation}
    \label{eq:gradient}
    \nabla f := \left(
      \frac{\partial f}{\partial x_1}, 
      \frac{\partial f}{\partial x_2}, 
      \ldots,
      \frac{\partial f}{\partial x_n}
      \right).
  \end{equation}  
\end{defn}

\begin{rem}
  By Lemma \ref{lem:totalDiffImpliesDirectionalDiff},
  one can write directional derivatives
  in terms of partial derivatives
  if the function is actually differentiable at that point.
  More precisely, for the vector
  $\mathbf{v}=(v_1, v_2, \ldots, v_n)^T=\sum_j v_j e_j$,
  we have
  \begin{displaymath}
    D_{\mathbf{v}} f(x_0) = f'(x_0) \sum_j v_j e_j = \sum_j v_j
    f'(x_0) e_j = \sum_j v_j \frac{\partial f}{\partial x_j}(x_0), 
  \end{displaymath}
  where the first step follows from Lemma
  \ref{lem:totalDiffImpliesDirectionalDiff},
  the second step from $f'(x_0)$ being a linear transformation,
  and the last step from (\ref{eq:partialDerivative}),
  (\ref{eq:totalDiffImpliesDirectionalDiff}), and
  (\ref{eq:directionalDerivative}).
  In particular, for $m=1$ we have
\end{rem}

\begin{coro}
  For a differentiable function
  $f: \mathbb{R}^n\rightarrow \mathbb{R}$,
  we have
  $D_{\mathbf{v}} f(x_0) = \mathbf{v}\cdot \nabla f(x_0)$.
\end{coro}

\begin{exc}
  Show that the existence of partial derivatives at $x_0$
  does not imply that the function is differentiable at $x_0$
  by considering the differentiability of
  the following function $f: \mathbb{R}^2\rightarrow \mathbb{R}$
  at (0,0). 
  \begin{displaymath}
    f(x,y) =
    \begin{cases}
      \frac{x^3}{x^2+y^2} & \text{if } (x,y)\ne (0,0);
      \\
      0 & \text{if } (x,y)= (0,0).
    \end{cases}
  \end{displaymath}
\end{exc}
\begin{solution}
The function is not differentiable at (0,0),
  although it is differentiable in every direction $\mathbf{v}\in
  \mathbb{R}^2$ at $(0,0)$.
  Indeed, for $\mathbf{v}=(1,0)$, we have
  \begin{displaymath}
    D_{\mathbf{v}} f (0,0) = \lim \frac{f(t,0)-f(0,0)}{t}
    = 1
  \end{displaymath}
  while for $\mathbf{u}=(0,1)$, we have
  \begin{displaymath}
    D_{\mathbf{u}} f (0,1) = \lim \frac{f(0,t)-f(0,0)}{t}
    = 0.
  \end{displaymath}
  Hence the limit in (\ref{eq:derivativeNdim}) does not exist.
\end{solution}

\begin{defn}
  The \emph{derivative matrix} or \emph{differential matrix}
  or \emph{Jacobian matrix} of a differentiable function
  \mbox{$f: \mathbb{R}^n\rightarrow\mathbb{R}^m$}
  is a $m\times n$ matrix,
  \begin{equation}
    \label{eq:JacobianMatrix}
    Df :=
    \begin{pmatrix}
      \frac{\partial f_1}{\partial x_1}
      &
      \frac{\partial f_1}{\partial x_2}
      & \cdots &
      \frac{\partial f_1}{\partial x_n}
      \\
      \frac{\partial f_2}{\partial x_1}
      &
      \frac{\partial f_2}{\partial x_2}
      & \cdots &
      \frac{\partial f_2}{\partial x_n}
      \\
      \vdots & \vdots & \ddots & \vdots &
      \\
      \frac{\partial f_m}{\partial x_1}
      &
      \frac{\partial f_m}{\partial x_2}
      & \cdots &
      \frac{\partial f_m}{\partial x_n}
    \end{pmatrix}.
  \end{equation}
\end{defn}

\begin{rem}
  For a row vector $\mathbf{v}\in \mathbb{R}^n$,
  we have
  \begin{displaymath}
%    \label{eq:relationsOfMatrixAndDerivatives}
    (D_{\mathbf{v}} f(x_0))^T = (f'(x_0)\mathbf{v})^T = D f(x_0) \mathbf{v}^T.
  \end{displaymath}
  We can also write $D f$ as
  \begin{align*}
    Df(x_0) &= \left(
      \frac{\partial f_1}{\partial x_1},
      \frac{\partial f_1}{\partial x_2},
      \ldots, 
      \frac{\partial f_1}{\partial x_n}\right)
    \\
    &=
      \begin{pmatrix}
        \nabla f_1(x_0)
        \\
        \nabla f_2(x_0)
        \\
        \vdots
        \\
        \nabla f_m(x_0)
      \end{pmatrix}.
  \end{align*}
\end{rem}

\begin{defn}
  \label{def:C1VecFunc}
  A differentiable function $\mathbf{f}: E\rightarrow \mathbb{R}^m$
  with $E\subset \mathbb{R}^n$ open
  is \emph{continuously differentiable},
  written $\mathbf{f}\in{\cal C}^1(E)$, 
  iff $\mathbf{f}'$ is a continuous mapping of $E$
  into ${\cal L}(\mathbb{R}^n,\mathbb{R}^m)$.
\end{defn}

\begin{thm}
  \label{thm:continuousParitialDerivativesImplyDifferentiability}
  Consider a function $\mathbf{f}: E\rightarrow \mathbb{R}^m$
  with $E\subset \mathbb{R}^n$ an open subset.
  Then $\mathbf{f}\in {\cal C}^1(E)$
  if and only if the partial derivatives $\frac{\partial f_i}{\partial x_j}$
  exist and are continuous on $E$ for all $i=1,\ldots,m$ and $j=1,\ldots,n$.
  In this case, the linear transformation
  $\mathbf{f}'(x_0): \mathbb{R}^n \rightarrow \mathbb{R}^m$ is given by
  \begin{equation}
    \label{eq:continuousParitialDerivativesImplyDifferentiability}
    \mathbf{f}'(x_0) (\mathbf{v})
    = \sum_{j=1}^n v_j \frac{\partial \mathbf{f}}{\partial x_j} (x_0).
  \end{equation}
\end{thm}



\subsection{Matrix exponential}
\label{sec:matrix-exponential}

\begin{rem}
  Most contents in this subsection %in Section \ref{sec:matrix-exponential}
  about matrix exponentials
  also apply to operator exponentials in Section \ref{sec:series-operators}.
\end{rem}

\begin{defn}
  \label{def:matrixExponential}
  The \emph{matrix exponential} $e^{A}$
  of a complex matrix $A\in \mathbb{C}^{n\times n}$ is 
  \begin{equation}
    \label{eq:matrixExponential}
    e^{A} 
    := \sum_{N=0}^{\infty}\frac{1}{N!}A^N
    = I + A + \frac{1}{2!}A^2 + \cdots
  \end{equation}
\end{defn}

\begin{rem}
  It follows from (\ref{eq:matrixExponentialFromDecomp})
  in Example \ref{exm:operatorDecompositionExp} that
  (\ref{eq:matrixExponential}) is well defined.
  Another justification is due to the following. 
\end{rem}

\begin{lem}
  \label{lem:expMatConverges}
  The series in (\ref{eq:matrixExponential}) is 
  \emph{entry-by-entry} convergent.
\end{lem}
\begin{proof}
  Apply the Hilbert-Schmidt norm
  in Definition \ref{def:HilbertSchmidtNorm}
  to (\ref{eq:matrixExponential})
  and we have
  \begin{displaymath}
    \left|e^{A}\right|
    = \left|\sum_{N=0}^{\infty}\frac{1}{N!}A^N\right|
    \le \sum_{N=0}^{\infty}\frac{1}{N!}\left|A^N\right|
    \le \sum_{N=0}^{\infty}\frac{1}{N!}\left|A\right|^N,
  \end{displaymath}
  where the second inequality follows from Corollary
  \ref{coro:compositionTriangleIneqHS}. 
  Since the size of $A$ is fixed,
  $|A|$ is a bounded nonnegative number.
  Then the ratio test (Theorem \ref{thm:ratioTest}) implies
   the convergence of the last series in the Hilbert-Schmidt norm,
   % which, by Theorem \ref{thm:opNormAndHSnorm},
   % is equivalent to the convergence in the Hilbert-Schmidt norm,
   which,
   by (\ref{eq:HilbertSchmidtNormMatrix}), %Definition \ref{def:HilbertSchmidtNorm},
   is equivalent to
   the entry-by-entry convergence.
\end{proof}

\begin{thm}
  \label{thm:matExpSmooth}
  Interpreted as a map $f: \mathbb{R}^{2n^2}\rightarrow \mathbb{R}^{2n^2}$,
   the matrix exponential $A\mapsto e^A$ is ${\cal C}^{\infty}$.
  In other words,
   every partial derivative of $f$
   to any order is entry-by-entry uniformly convergent
   to some continuous function.
\end{thm}
\begin{proof}
  Denote by $E_j$, $j=1,\ldots,2n^2$, 
  an $n$-by-$n$ complex matrix that has 1 or $\ii$ in one entry
  and 0 in all other entries.
  By Definition \ref{def:partialDerivative},
  the partial derivative of $f$ in the direction of $E_j$
  is
  \begin{displaymath}
    \frac{\partial f}{\partial E_j}(A)
    = \left. \frac{\dif }{\dif t} f(A+tE_j)\right|_{t=0}.
  \end{displaymath}
  By (\ref{eq:matrixExponential}) and Definition \ref{def:seriesFromSequence},
  the sequence associated with the series $f(A)$ is
  $\left\{\frac{1}{N!}A^N\right\}$.
%  $g_1(A)g_2(A)\cdots g_N(A)$
%  where each $g_j(A)$ equal to $A$ or a constant.
  Hence 
  $\frac{\partial f}{\partial E_j}(A)$ is the sum
  of derivatives of all terms in the sequence; 
  by the chain rule, 
  each derivative is of the form
  \begin{displaymath}
    \frac{1}{N!}\sum_{i=1}^N g_1(A)\cdots g_{i-1}(A)
    \left. \frac{\dif }{\dif t} g_i(A+tE_j)\right|_{t=0}
    g_{i+1}(A)\cdots g_N(A),
  \end{displaymath}
  where each $g_i$ is $A$. % or $E_j$. % or a constant.
  Taking further partial derivatives preserves this general form,
  except that $g_i$ is either $A$ or $E_j$
  and that the number of products to be summed up is increased.
  
  % $\left\{\frac{1}{N!}A^N\right\}$,
  The $k$th-order partial derivative of
  the $N$th term $\frac{1}{N!}A^N$ in the sequence
  is a sum of $N^k$ products,
  each product consisting of $N$ terms
  and each term is either $A$ or $E_j$
  satisfying
  \begin{displaymath}
    \max(|A|, |E_j|)\le M := \max(|A|,1).
  \end{displaymath}
  Hence we have, for any fixed $k\in \mathbb{N}$, 
  \begin{displaymath}
    \left|
      \frac{\partial^k }{\partial E_{j_1}\cdots \partial E_{j_k}}
      \left(\frac{1}{N!}A^N\right)
    \right|
    \le \frac{N^k M^N}{N!}.
  \end{displaymath}
  By the ratio test, 
  the series $\frac{\partial^k f}{\partial E_{j_1}\cdots
    \partial E_{j_k}}(A)$
  uniformly converges entry-by-entry to some function. 
  The rest of the proof follows from
  Theorem \ref{thm:uniformConvergenceOfDerivatives}
  and Lemma \ref{lem:expMatConverges}.
\end{proof}

\begin{exm}
  For the real skew-symmetric matrix
  \begin{displaymath}
    A =
    \begin{bmatrix}
      0 & -\theta
      \\
      \theta & 0
    \end{bmatrix},
  \end{displaymath}
  we have
  \begin{displaymath}
    e^A =
    \begin{bmatrix}
      \cos \theta & -\sin\theta
      \\
      \sin\theta & \cos\theta
    \end{bmatrix}.
  \end{displaymath}
  Indeed, define
  \begin{displaymath}
    I_2 :=
    \begin{bmatrix}
      1 & 0
      \\
      0 & 1
    \end{bmatrix};
    \quad
    J_2 :=
    \begin{bmatrix}
      0 & -1
      \\
      1 & 0
    \end{bmatrix}
  \end{displaymath}
  and we have
  \begin{align*}
    A^{4n} &= \theta^{4n} I_2, \ 
    A^{4n+2} = -\theta^{4n+2} I_2, \\
    A^{4n+1} &= \theta^{4n+1} J_2, \ 
    A^{4n+3} = -\theta^{4n+3} J_2. 
  \end{align*}
  It follows that
  $e^A = \cos\theta I_2 + \sin\theta J_2$. 
\end{exm}

\begin{lem}
  \label{lem:exp-commuteMatrices}
  If two matrices $X$ and $Y$ commute,
  then
  \begin{equation}
    \label{eq:exp-commuteMatrices}
    e^X e^Y = e^{X+Y}.
  \end{equation}
\end{lem}
\begin{proof}
  By rearranging double summations, we have
  \begin{align*}
    e^X e^Y
    &= \left(
      \sum_{r=0}^{\infty} \frac{1}{r!} X^r
      \right)
      \left(
      \sum_{s=0}^{\infty} \frac{1}{s!} Y^s
      \right)
      = \sum_{r,s\in \mathbb{N}} \frac{1}{r! s!} X^rY^s
    \\
    &= \sum_{N=0}^{\infty}\sum_{k=0}^N \frac{X^k Y^{N-k}}{k!(N-k)!} 
      = \sum_{N=0}^{\infty}\frac{1}{N!}
      \sum_{k=0}^N {N \choose k} X^k Y^{N-k}
    \\
    &= \sum_{N=0}^{\infty}\frac{1}{N!} (X+Y)^N
      = e^{X+Y},
  \end{align*}
  where the commutativity of $X$ and $Y$ ensures the validity of the
  last two steps.
\end{proof}

\begin{exm}
  If two matrices $X$ and $Y$ do not commute,
  then Lemma \ref{lem:exp-commuteMatrices} does not hold,
  e.g., 
  \begin{displaymath}
    X=
    \begin{bmatrix}
      0 & 1 \\ 0 & 0
    \end{bmatrix}, \ 
    Y=
    \begin{bmatrix}
      1 & 0 \\ 0 & 0
    \end{bmatrix}.
  \end{displaymath}
\end{exm}

\begin{coro}
  The matrix $e^X$ is nonsingular
  for any $X\in \mathbb{C}^{n\times n}$.
\end{coro}
\begin{proof}
  This follows from 
  setting $Y=-X$ in (\ref{eq:exp-commuteMatrices})
  and taking determinant of both sides.
\end{proof}

\begin{lem}
  \label{lem:derivativeOfMatrixExponential}
  $\frac{\dif}{\dif t}(e^{tX}) = X e^{tX}$.
\end{lem}
\begin{proof}
  By (\ref{eq:matrixExponential}) and Theorem \ref{thm:matExpSmooth}, we have
  \begin{align*}
    \frac{\dif}{\dif t}\left(e^{tX}\right)
    &= \frac{\dif}{\dif t}\sum_{N=0}^{\infty} \frac{1}{N!}(tX)^N
    = X\sum_{N=1}^{\infty} \frac{1}{(N-1)!}(tX)^{N-1}
    \\
    &= X e^{tX}. \qedhere
  \end{align*}
\end{proof}

\begin{lem}
  \label{lem:expSimilarityTrans}
  For any nonsingular matrix $W$, we have 
  \begin{equation}
    \label{eq:expSimilarityTrans}
    e^{W^{-1}X W} = W^{-1} e^X W.
  \end{equation}
\end{lem}
\begin{proof}
  By (\ref{eq:matrixExponential}), we have
  \begin{align*}
    e^{W^{-1}X W} &= \sum_{N=0}^{\infty} \frac{1}{N!}(W^{-1}X W)^N
    = \sum_{N=0}^{\infty} \frac{1}{N!}W^{-1}X^N W
    \\
    &= W^{-1} e^X  W. \qedhere
  \end{align*}  
\end{proof}

\begin{coro}
  \label{coro:diagonalMatrixExp}
  For a diagonalizable matrix $A=R \Lambda R^{-1}$, we have
  \begin{equation}
    \label{eq:diagonalMatrixExp}
    e^A = R e^{\Lambda} R^{-1}.
  \end{equation}
  % \begin{align*}
  %   e^{A t} &=RR^{-1}+R\Lambda R^{-1}t+\frac{1}{2!}R\Lambda R^{-1}R\Lambda R^{-1}t^2+\cdots \\
  %           &= R\sum_{j=0}^{\infty}\frac{t^j}{j!}\Lambda^jR^{-1}=Re^{\Lambda t}R^{-1}.
  % \end{align*}
\end{coro}
\begin{proof}
  This follows directly from Lemma \ref{lem:expSimilarityTrans}.
\end{proof}

\begin{lem}
  \label{lem:eigenPairOfExp}  
  If $\lambda_1, \ldots, \lambda_n$ are eigenvalues of
  $A\in \mathbb{C}^{n\times n}$,
  then $e^{\lambda_1}, \ldots, e^{\lambda_n}$
  are eigenvalues of $e^A$.
  Furthermore, if $\mathbf{u}$ is an eigenvector
  of $A$ for $\lambda_i$,
  then $\mathbf{u}$ is an eigenvector
  of $e^A$ for $e^{\lambda_i}$.
\end{lem}
\begin{proof}
  By the Schur Theorem \ref{thm:Schur},
  there exist an invertible matrix $P$
  and an upper triangular matrix $T$ such that
  \begin{displaymath}
    A = P^{-1} T P.
  \end{displaymath}
  Then Lemma \ref{lem:expSimilarityTrans}
  yields
  \begin{displaymath}
    e^A = e^{P^{-1} T P} = P^{-1} e^{T} P,  
  \end{displaymath}
  where $e^T$, by Definition \ref{def:matrixExponential},
  is an upper triangular matrix with
  its diagonal entries as
  $e^{\lambda_1}, \ldots, e^{\lambda_n}$.
  If $\mathbf{u}$ is an eigenvector
  of $A$ for the eigenvalue $\lambda$,
  then $\mathbf{u}$ is an eigenvector
  of $A^n$ for the eigenvalue $\lambda^n$, 
  the rest follows from Definition \ref{def:matrixExponential}. 
\end{proof}

\begin{thm}
  \label{thm:expMatDet}
  $\det e^X = e^{\text{Trace } X}$.
\end{thm}

\begin{exc}
  Prove Theorem \ref{thm:expMatDet}.
\end{exc}
\begin{solution}
  Let $\lambda_1, \ldots, \lambda_n$
  be the eigenvalues of $X$. 
  Lemmas \ref{lem:eigenPairOfExp},
  \ref{lem:trace},
  and \ref{lem:determinantIsProductOfEigenvalues} yield
  \begin{displaymath}
    \det e^X = e^{\lambda_1} e^{\lambda_2}\cdots e^{\lambda_n}
    = e^{\lambda_1+\lambda_2+\ldots+\lambda_n} = e^{\text{Trace } X}. 
    \qedhere
  \end{displaymath}
\end{solution}


\begin{rem}
  An alternative proof of Theorem \ref{thm:expMatDet} is as follows. 
  Define $f(t) = \det e^{tX}$.
  By (\ref{eq:matrixExponential}), we have
  \begin{align*}
    &\det e^{sX} \\
    =& \det
    \begin{pmatrix}
      1 + s x_{11} + s^2 F_{11}(s)  & s x_{12} + s^2 F_{12}(s) &
      \cdots
      \\
      s x_{21} + s^2 F_{21}(s)  & 1 + s x_{22} + s^2 F_{22}(s) &
      \cdots
      \\
      \vdots & \vdots & \ddots 
    \end{pmatrix}
    \\
    =& 1 + s \text{Trace } X + s^2 G(s),
  \end{align*}
  where $x_{ij}$ is the $(i,j)$-entry of $X$
  and $F_{ij}(s)$ and $G(s)$ are smooth functions of $s$.
  Consequently, we have
  \begin{align*}
    f'(t)
    &= \left. \frac{\dif }{\dif s} \det e^{(t+s)X}\right|_{s=0}
      = \left. \frac{\dif }{\dif s}
      \det \left(e^{tX}e^{sX}\right)\right|_{s=0}
    \\
    &= \left. \frac{\dif }{\dif s}
      \left(\det e^{tX} \det e^{sX}\right)\right|_{s=0}
      = \det e^{tX} \left. \frac{\dif }{\dif s}
      \left(\det e^{sX}\right)\right|_{s=0}
    \\
    &= f(t) \left. \frac{\dif }{\dif s} \left(
      1 + s \text{Trace } X + s^2 G(s)
      \right)\right|_{s=0}
    \\
    &= f(t) \text{ Trace } X,
  \end{align*}
  where the second step follows from Lemma
  \ref{lem:exp-commuteMatrices} and 
  the third step from a property of determinant.
  It follows that  
  \begin{align*}
    &\frac{\dif}{\dif t} \left( e^{- t \text{ Trace } X}f(t)\right)
    \\
    =& e^{- t \text{ Trace } X}f'(t) -
    e^{- t \text{ Trace } X} f(t) \text{ Trace } X  = 0
  \end{align*}
  and hence $e^{- t \text{ Trace } X}f(t)$ is a constant,
  which is clearly 1 from setting $t=0$.
  Finally, the identity
  \begin{displaymath}
    f(t) = \det e^{tX} = e^{t \text{ Trace } X}
  \end{displaymath}
  yields Theorem \ref{thm:expMatDet} at $t=1$.
\end{rem}

\begin{rem}
  The operator norm and matrix exponential in this chapter
  concerns linear maps that act on finite-dimensional spaces.
  For infinite-dimensional spaces,
  see Section \ref{sec:operator-norms} and \ref{sec:series-operators}.
\end{rem}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../numPDEs2cols"
%%% End:

% LocalWords:  Lipschitz pointwise boundedness integrable bijective
% LocalWords:  infimum supremum iff subsequence 
